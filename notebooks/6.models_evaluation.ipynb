{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93956c07-d9f1-4e3a-ae1b-148aec301334",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "# import pandas as pd\n",
    "# print(pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100697fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizations\n",
    "# GDAL optimizations\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "cpu_count: int = mp.cpu_count()\n",
    "num_cores: int = cpu_count - 2\n",
    "os.environ[\"GDAL_NUM_THREADS\"] = f\"{num_cores}\"\n",
    "os.environ[\"GDAL_CACHEMAX\"] = \"1024\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2a25b-11a9-436c-9eff-17ad4003f00c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13a12a-7502-4c76-b490-9880fbf99c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import riskmapjnr as rmj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c369402",
   "metadata": {},
   "source": [
    "## Dask Processing Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "cluster = LocalCluster(n_workers=8, threads_per_worker=2, memory_limit=\"2GB\")\n",
    "geospatial_client = Client(cluster)\n",
    "geospatial_client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15850cf5-6362-4e08-aef7-f9372ff90576",
   "metadata": {},
   "source": [
    "## Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a76eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2020, 2024]\n",
    "tree_cover_threshold = 10\n",
    "forest_source = \"gfc\"  ##gfc, tmf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc668b-a41e-4bbe-9942-39610df8a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_grid_cell_size_pixels: list[int] = [300]\n",
    "models_to_compare: list[str] = [\"rmj_bm\", \"rmj_mw\", \"far_rf\", \"far_icar\"]\n",
    "periods: list[str] = [\"calibration\", \"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c312be-754e-43c3-b465-40c41963b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coarse_grid_cell_size_pixels = [50, 100]\n",
    "# models_to_compare = [\"rmj_bm\", \"rmj_mw\", \"rf\", \"icar\", \"glm\", \"user\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d8fcd-6d62-4b47-a589-5921a4cc8102",
   "metadata": {},
   "source": [
    "## Connect folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder: Path = Path.cwd().parent\n",
    "downloads_folder: Path = root_folder / \"data\"\n",
    "downloads_folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f775e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = downloads_folder / project_name\n",
    "project_folder.mkdir(parents=True, exist_ok=True)\n",
    "processed_data_folder = project_folder / \"data\"\n",
    "processed_data_folder.mkdir(parents=True, exist_ok=True)\n",
    "evaluation_folder = project_folder / \"evaluation\"\n",
    "evaluation_folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff58466-9b05-4d17-b733-bcd2c523a01d",
   "metadata": {},
   "source": [
    "## Select predictions files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99809c-f245-40c8-a058-e0b39b7ffb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders(directory):\n",
    "    \"\"\"\n",
    "    Lists all folders (directories) within a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): The path to the directory from which to list folders.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of folder names within the specified directory.\n",
    "              If an error occurs, returns an empty list and prints an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a Path object for the directory\n",
    "        path = Path(directory)\n",
    "\n",
    "        # Filter out only directories (folders) using is_dir()\n",
    "        folders = [entry for entry in path.iterdir() if entry.is_dir()]\n",
    "\n",
    "        return folders\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac38001-5c03-4f0c-bb35-74f53050b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_folders(input_folders, filter_words, exclude_words=None):\n",
    "    \"\"\"\n",
    "    Filters a list of folders based on include and exclude words.\n",
    "    Parameters:\n",
    "        input_folders (list): List of folder names to be filtered.\n",
    "        filter_words (list): Words that must be present in the folder names for inclusion.\n",
    "        exclude_words (list, optional): Words that must not be present in the folder names for exclusion. Defaults to None.\n",
    "    Returns:\n",
    "        list: Filtered list of folders.\n",
    "    \"\"\"\n",
    "    # Ensure all words are lowercase for case-insensitive comparison\n",
    "    filter_words = [word.lower() for word in filter_words]\n",
    "    exclude_words = [word.lower() for word in (exclude_words or [])]\n",
    "\n",
    "    filtered_folders = [\n",
    "        folder\n",
    "        for folder in input_folders\n",
    "        if any(word in folder.name.lower() for word in filter_words)\n",
    "        and not any(\n",
    "            exclude_word in folder.name.lower() for exclude_word in exclude_words\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return filtered_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e38df2-acf5-4572-a2cc-8a02782be7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_by_extension(folder_path, file_extensions, recursive=False):\n",
    "    \"\"\"\n",
    "    List all files with specified extensions in the given folder.\n",
    "    Parameters:\n",
    "    folder_path (str or Path): The path to the folder where you want to search for files.\n",
    "    file_extensions (list of str): A list of file extensions to search for (e.g., ['.shp', '.tif']).\n",
    "    recursive (bool): Whether to recursively search through subdirectories or not.\n",
    "    Returns:\n",
    "    list: A list of file paths with the specified extensions.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "    try:\n",
    "        # Convert folder_path to Path object if it's a string\n",
    "        folder_path = Path(folder_path)\n",
    "\n",
    "        # Check if the provided path is a directory\n",
    "        if folder_path.is_dir():\n",
    "            for entry in folder_path.iterdir():\n",
    "                if entry.is_file() and any(\n",
    "                    entry.suffix.lower() == ext.lower() for ext in file_extensions\n",
    "                ):\n",
    "                    matching_files.append(str(entry))\n",
    "                elif recursive and entry.is_dir():\n",
    "                    # Recursively search subdirectories\n",
    "                    matching_files.extend(\n",
    "                        list_files_by_extension(entry, file_extensions, recursive)\n",
    "                    )\n",
    "        else:\n",
    "            print(f\"The provided path '{folder_path}' is not a directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return matching_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6104aeb-e4cf-45a0-99dd-2d6e248daee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files(input_files, filter_words, exclude_words=None, include_all=True):\n",
    "    \"\"\"\n",
    "    Filters a list of files based on include and exclude words.\n",
    "    Parameters:\n",
    "        input_files (list): List of file paths to be filtered.\n",
    "        filter_words (list): Words that must be present in the filenames for inclusion.\n",
    "        exclude_words (list, optional): Words that must not be present in the filenames for exclusion. Defaults to None.\n",
    "        include_all (bool, optional): If True, all filter words must be present in the filename. If False, at least one of the filter words must be present. Defaults to False.\n",
    "    Returns:\n",
    "        list: Filtered list of files.\n",
    "    \"\"\"\n",
    "    # Ensure all words are lowercase for case-insensitive comparison\n",
    "    filter_words = [word.lower() for word in filter_words]\n",
    "    exclude_words = [word.lower() for word in (exclude_words or [])]\n",
    "\n",
    "    if include_all:\n",
    "        filtered_files = [\n",
    "            file\n",
    "            for file in input_files\n",
    "            if all(word in Path(file).name.lower() for word in filter_words)\n",
    "            and not any(\n",
    "                exclude_word in Path(file).name.lower()\n",
    "                for exclude_word in exclude_words\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        filtered_files = [\n",
    "            file\n",
    "            for file in input_files\n",
    "            if any(word in Path(file).name.lower() for word in filter_words)\n",
    "            and not any(\n",
    "                exclude_word in Path(file).name.lower()\n",
    "                for exclude_word in exclude_words\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return filtered_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7e6b7-a73b-4b6d-a699-576996f7c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_ipynb_checkpoints(input_files):\n",
    "    \"\"\"\n",
    "    Filters out files whose paths contain '.ipynb_checkpoints'.\n",
    "    Parameters:\n",
    "        input_files (list): List of file paths to be filtered.\n",
    "    Returns:\n",
    "        list: Filtered list of files.\n",
    "    \"\"\"\n",
    "    filtered_files = [\n",
    "        file for file in input_files if \".ipynb_checkpoints\" not in Path(file).parts\n",
    "    ]\n",
    "    return filtered_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd71cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = project_folder\n",
    "folders = list_folders(directory_path)\n",
    "available_models = filter_folders(folders, models_to_compare, [\"data\", \"data_raw\"])\n",
    "print(\"Models_available:\", available_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c67c32-6e2f-4a17-89ef-d0182fa46122",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = list_folders(project_folder)\n",
    "available_models = filter_folders(folders, models_to_compare, [\"data\", \"data_raw\"])\n",
    "\n",
    "available_prediction_files = []\n",
    "for model_folder in available_models:\n",
    "    tif_files = list_files_by_extension(model_folder, [\".tif\"], True)\n",
    "    model_files = filter_files(tif_files, periods, None, False)\n",
    "    available_prediction_files.append(model_files)\n",
    "\n",
    "available_defrate_files = []\n",
    "for model_folder in available_models:\n",
    "    csv_files = list_files_by_extension(model_folder, [\".csv\"], True)\n",
    "    defrate_files = filter_files(csv_files, periods, None, False)\n",
    "    defrate_files1 = filter_files(defrate_files, [\"defrate\"])\n",
    "    defrate_files2 = filter_out_ipynb_checkpoints(defrate_files1)\n",
    "    available_defrate_files.append(defrate_files2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d7084-ac1a-4521-97c6-c84b13d7422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_prediction_files = sum(available_prediction_files, [])\n",
    "available_defrate_files = sum(available_defrate_files, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730627c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries mapping common names to paths\n",
    "dict1 = {}\n",
    "dict2 = {}\n",
    "\n",
    "\n",
    "# Function to extract the subsystem/model part from path\n",
    "def extract_subsystem(path):\n",
    "    \"\"\"Extract the subsystem identifier from path structure\"\"\"\n",
    "    path_obj = Path(path)\n",
    "\n",
    "    # Get the path components and look for key directories like rmj_bm, rmj_mw, etc.\n",
    "    # The pattern is: .../test/{subsystem}/{validation|calibration}/...\n",
    "    path_parts = path_obj.parts\n",
    "\n",
    "    # Find the subsystem by looking for test directory and its immediate subdirectory\n",
    "    try:\n",
    "        test_index = path_parts.index(\"test\")\n",
    "        if test_index + 1 < len(path_parts):\n",
    "            return path_parts[\n",
    "                test_index + 1\n",
    "            ]  # This should be the subsystem like rmj_bm\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to extract the period (validation/calibration/historical) from path\n",
    "def extract_period(path):\n",
    "    \"\"\"Extract the period identifier from path structure\"\"\"\n",
    "    path_obj = Path(path)\n",
    "\n",
    "    # Get the path components\n",
    "    path_parts = path_obj.parts\n",
    "\n",
    "    # Find the period by looking for validation or calibration directories\n",
    "    try:\n",
    "        for i, part in enumerate(path_parts):\n",
    "            if part in [\"validation\", \"calibration\", \"historical\"]:\n",
    "                return part\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "for path in available_prediction_files:\n",
    "    path_obj = Path(path)\n",
    "    filename = path_obj.name\n",
    "    subsystem = extract_subsystem(path)\n",
    "    period = extract_period(path)\n",
    "\n",
    "    name_no_ext = Path(filename).stem\n",
    "    if name_no_ext.startswith(\"prob_\"):\n",
    "        identifier = name_no_ext[len(\"prob_\") :]\n",
    "    else:\n",
    "        identifier: str = name_no_ext\n",
    "    key = (identifier, subsystem, period)\n",
    "    if key not in dict1:\n",
    "        dict1[key] = []\n",
    "    dict1[key].append(path)\n",
    "\n",
    "for path in available_defrate_files:\n",
    "    path_obj = Path(path)\n",
    "    filename = path_obj.name\n",
    "    subsystem = extract_subsystem(path)\n",
    "    period = extract_period(path)\n",
    "\n",
    "    name_no_ext = Path(filename).stem\n",
    "    identifier = name_no_ext[len(\"defrate_cat_\") :]\n",
    "\n",
    "    key = (identifier, subsystem, period)\n",
    "    if key not in dict2:\n",
    "        dict2[key] = []\n",
    "    dict2[key].append(path)\n",
    "\n",
    "\n",
    "# Create the final matching dictionary - now we match by both identifier and subsystem\n",
    "models_dict = {}\n",
    "for key1 in dict1:\n",
    "    if key1 in dict2:\n",
    "        # Create all possible combinations between tiff and csv files with same identifier and subsystem\n",
    "        identifier, subsystem, period = key1\n",
    "        for tiff_path in dict1[key1]:\n",
    "            for csv_path in dict2[key1]:\n",
    "                models_dict[(identifier, subsystem, period)] = (\n",
    "                    tiff_path,\n",
    "                    csv_path,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final matching dictionary with all attributes:\")\n",
    "for key, value in models_dict.items():\n",
    "    identifier, subsystem, period = key\n",
    "    tiff_path, csv_path = value\n",
    "    print(f\"Identifier '{identifier}', Subsystem '{subsystem}', Period '{period}':\")\n",
    "    print(f\"  TIFF: {tiff_path}\")\n",
    "    print(f\"  CSV:  {csv_path}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba919a-3341-442e-924d-ee870327731d",
   "metadata": {},
   "source": [
    "## Select forest cover change file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d17f5-7b11-481e-8cde-a49d0b51d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all raster files in the processed data folder\n",
    "input_raster_files = list_files_by_extension(processed_data_folder, [\".tiff\", \".tif\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9d584-29bd-436a-a7f1-aa3a9a0e030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_change_file = filter_files(\n",
    "    input_raster_files,\n",
    "    [\"forest\", \"loss\", forest_source] + [str(num) for num in years],\n",
    "    [\"distance\", \"edge\"],\n",
    ")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dad12b-f99b-4c5e-a77c-0af59155e580",
   "metadata": {},
   "source": [
    "## Periods dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be2707-82be-4def-be45-ca8a61ce8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_dict = {\n",
    "    \"period\": \"calibration\",\n",
    "    \"initial_year\": years[0],\n",
    "    \"final_year\": years[1],\n",
    "    \"defor_value\": 1,\n",
    "    \"time_interval\": years[1] - years[0],\n",
    "}\n",
    "validation_dict = {\n",
    "    \"period\": \"validation\",\n",
    "    \"initial_year\": years[1],\n",
    "    \"final_year\": years[2],\n",
    "    \"defor_value\": 1,\n",
    "    \"time_interval\": years[2] - years[1],\n",
    "}\n",
    "historical_dict = {\n",
    "    \"period\": \"historical\",\n",
    "    \"initial_year\": years[0],\n",
    "    \"final_year\": years[2],\n",
    "    \"defor_value\": [1, 2],\n",
    "    \"time_interval\": years[2] - years[0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77fdb0-3ae6-4b22-92e3-f5ddb76869be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el diccionario principal\n",
    "period_dict = {\n",
    "    calibration_dict[\"period\"]: calibration_dict,\n",
    "    validation_dict[\"period\"]: validation_dict,\n",
    "    historical_dict[\"period\"]: historical_dict,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c886152-5c18-4daf-884b-45434df2b29a",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import dask\n",
    "from dask import delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def validation_udef_arp_distributed(\n",
    "    fcc_file,\n",
    "    time_interval,\n",
    "    riskmap_file,\n",
    "    tab_file_defor,\n",
    "    period=\"calibration\",\n",
    "    csize_coarse_grid=300,\n",
    "    indices_file_pred=\"indices.csv\",\n",
    "    tab_file_pred=\"pred_obs.csv\",\n",
    "    fig_file_pred=\"pred_obs.png\",\n",
    "    figsize=(6.4, 6.4),\n",
    "    dpi=100,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Validation of the deforestation risk map with distributed processing.\n",
    "\n",
    "    This function computes the observed and predicted deforestation (in ha)\n",
    "    for either the calibration or validation period using distributed computing\n",
    "    through dask and rioxarray. The function creates both a .csv file with\n",
    "    the validation data and a plot comparing predictions vs observations.\n",
    "\n",
    "    :param fcc_file: Input raster file of forest cover change at three dates (123).\n",
    "        1: first period deforestation, 2: second period deforestation, 3: remaining forest\n",
    "        at the end of the second period. No data value must be 0 (zero).\n",
    "\n",
    "    :param period: Either \"calibration\" (from t1 to t2), \"validation\" (from t2 to t3),\n",
    "        or \"historical\" (from t1 to t3).\n",
    "\n",
    "    :param time_interval: Duration (in years) of the period.\n",
    "\n",
    "    :param riskmap_file: Input raster file with categories of spatial deforestation\n",
    "        risk at the beginning of the period.\n",
    "\n",
    "    :param tab_file_defor: Path to the .csv input file with estimates of deforestation\n",
    "        density (in ha/pixel/yr) for each category of deforestation risk.\n",
    "\n",
    "    :param csize_coarse_grid: Spatial cell size in number of pixels. Must correspond to a\n",
    "        distance < 10 km. Default to 300 corresponding to 9 km for a 30 m resolution raster.\n",
    "\n",
    "    :param tab_file_pred: Path to the .csv output file with validation data.\n",
    "\n",
    "    :param fig_file_pred: Path to the .png output file for the predictions vs observations plot.\n",
    "\n",
    "    :param figsize: Figure size.\n",
    "\n",
    "    :param dpi: Resolution for output image.\n",
    "\n",
    "    :param verbose: Logical. Whether to print messages or not. Default to True.\n",
    "\n",
    "    :return: A dictionary. With wRMSE, MedAE, and R2: weighted root mean squared error\n",
    "        (in ha), median absolute error (in ha), and R-square respectively for the\n",
    "        deforestation predictions, ncell: the number of grid cells with forest cover > 0\n",
    "        at the beginning of the validation period, csize_coarse_grid: the coarse grid\n",
    "        cell size in number of pixels, csize_coarse_grid_ha: the coarse grid cell size in ha.\n",
    "    \"\"\"\n",
    "\n",
    "    # ==============================================================\n",
    "    # Input data - Using rioxarray for improved geospatial handling\n",
    "    # ==============================================================\n",
    "\n",
    "    # Open raster files using rioxarray (supports lazy loading and chunking)\n",
    "    fcc_ds = rxr.open_rasterio(\n",
    "        fcc_file, chunks={\"x\": csize_coarse_grid, \"y\": csize_coarse_grid}\n",
    "    )\n",
    "    defor_cat_ds = rxr.open_rasterio(\n",
    "        riskmap_file, chunks={\"x\": csize_coarse_grid, \"y\": csize_coarse_grid}\n",
    "    )\n",
    "\n",
    "    # Get defor_dens per cat\n",
    "    defor_dens_per_cat = pd.read_csv(tab_file_defor)\n",
    "    cat = defor_dens_per_cat[\"cat\"].values\n",
    "\n",
    "    # Pixel area (in unit square, eg. meter square)\n",
    "    pix_area = fcc_ds.attrs.get(\"pixel_size\", 1) * abs(\n",
    "        fcc_ds.attrs.get(\"pixel_height\", 1)\n",
    "    )\n",
    "\n",
    "    # If we don't have pixel size info from the raster, calculate it\n",
    "    if pix_area == 1:\n",
    "        # Get geotransform info if available\n",
    "        try:\n",
    "            gt = fcc_ds.rio.transform()\n",
    "            pix_area = abs(gt[0] * gt[4])  # Pixel area in meters squared\n",
    "        except:\n",
    "            pix_area = 900  # Default to 30m x 30m pixels\n",
    "\n",
    "    # Get the data arrays (squeeze to remove any time dimensions)\n",
    "    fcc_data = fcc_ds.squeeze()\n",
    "    defor_cat_data = defor_cat_ds.squeeze()\n",
    "\n",
    "    # ==============================================================\n",
    "    # Distributed Processing using dask delayed functions\n",
    "    # ==============================================================\n",
    "\n",
    "    @delayed\n",
    "    def process_chunk(\n",
    "        fcc_chunk,\n",
    "        defor_cat_chunk,\n",
    "        defor_dens_per_cat,\n",
    "        cat,\n",
    "        period,\n",
    "        time_interval,\n",
    "        pix_area,\n",
    "    ):\n",
    "        \"\"\"Process a single chunk of data.\"\"\"\n",
    "        # Calculate observed values\n",
    "        if period == \"calibration\":\n",
    "            nfor_obs = int(fcc_chunk.where(fcc_chunk > 0).count().values)\n",
    "            ndefor_obs = int(fcc_chunk.where(fcc_chunk == 1).count().values)\n",
    "        elif period == \"validation\":\n",
    "            nfor_obs = int(fcc_chunk.where(fcc_chunk > 1).count().values)\n",
    "            ndefor_obs = int(fcc_chunk.where(fcc_chunk == 2).count().values)\n",
    "        else:  # historical\n",
    "            nfor_obs = int(fcc_chunk.where(fcc_chunk > 0).count().values)\n",
    "            ndefor_obs = int(\n",
    "                fcc_chunk.where((fcc_chunk == 1) | (fcc_chunk == 2)).count().values\n",
    "            )\n",
    "\n",
    "        # Calculate predicted deforestation\n",
    "        defor_cat_flat = defor_cat_chunk.values.flatten()\n",
    "        defor_cat_series = pd.Series(defor_cat_flat)\n",
    "\n",
    "        # Count occurrences of each category in this chunk\n",
    "        cat_counts = defor_cat_series.value_counts()\n",
    "\n",
    "        # Get deforestation density for each category\n",
    "        defor_dens = defor_dens_per_cat[\"defor_dens\"].values\n",
    "        defor_dens_period = defor_dens * time_interval\n",
    "\n",
    "        # Calculate predicted deforestation area in ha\n",
    "        ndefor_pred_ha = 0.0\n",
    "        for cat_val, count in cat_counts.items():\n",
    "            if cat_val in cat:\n",
    "                idx = np.where(cat == cat_val)[0][0]\n",
    "                ndefor_pred_ha += count * defor_dens_period[idx] * pix_area / 10000\n",
    "\n",
    "        # Return results for this chunk\n",
    "        return {\n",
    "            \"nfor_obs\": nfor_obs,\n",
    "            \"ndefor_obs\": ndefor_obs,\n",
    "            \"nfor_obs_ha\": nfor_obs * pix_area / 10000,\n",
    "            \"ndefor_obs_ha\": ndefor_obs * pix_area / 10000,\n",
    "            \"ndefor_pred_ha\": ndefor_pred_ha,\n",
    "        }\n",
    "\n",
    "    # Create delayed tasks for all chunks (dask will handle the chunking)\n",
    "    tasks = []\n",
    "\n",
    "    # Get all chunks from the dask arrays - this leverages the chunking already set up\n",
    "    # Dask will automatically distribute processing of these chunks across available cores\n",
    "    for i in range(len(fcc_data.chunks[0])):  # Iterate over y chunks\n",
    "        for j in range(len(fcc_data.chunks[1])):  # Iterate over x chunks\n",
    "            # Extract chunk data\n",
    "            y_start = sum(fcc_data.chunks[0][:i])\n",
    "            y_end = y_start + fcc_data.chunks[0][i]\n",
    "            x_start = sum(fcc_data.chunks[1][:j])\n",
    "            x_end = x_start + fcc_data.chunks[1][j]\n",
    "\n",
    "            fcc_chunk = fcc_data.isel(y=slice(y_start, y_end), x=slice(x_start, x_end))\n",
    "            defor_cat_chunk = defor_cat_data.isel(\n",
    "                y=slice(y_start, y_end), x=slice(x_start, x_end)\n",
    "            )\n",
    "\n",
    "            task = process_chunk(\n",
    "                fcc_chunk,\n",
    "                defor_cat_chunk,\n",
    "                defor_dens_per_cat,\n",
    "                cat,\n",
    "                period,\n",
    "                time_interval,\n",
    "                pix_area,\n",
    "            )\n",
    "            tasks.append(task)\n",
    "\n",
    "    # Compute all tasks in parallel\n",
    "    if verbose:\n",
    "        print(\"Processing chunks in parallel...\")\n",
    "\n",
    "    results = dask.compute(*tasks)\n",
    "\n",
    "    # ==============================================================\n",
    "    # Combine results and compute validation metrics\n",
    "    # ==============================================================\n",
    "\n",
    "    # Create DataFrame from results\n",
    "    df_data = []\n",
    "    for result in results:\n",
    "        if result[\"nfor_obs\"] > 0:  # Only include cells with forest cover\n",
    "            df_data.append(result)\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    # If no cells with forest cover, return error\n",
    "    if df.empty:\n",
    "        raise ValueError(\n",
    "            \"No grid cells with forest cover found. Please decrease the spatial cell size 'csize_coarse_grid'.\"\n",
    "        )\n",
    "\n",
    "    ncell = len(df)\n",
    "\n",
    "    # Cell size in ha\n",
    "    csize_coarse_grid_ha = round(\n",
    "        csize_coarse_grid * csize_coarse_grid * pix_area / 10000, 2\n",
    "    )\n",
    "\n",
    "    # Export the table of results\n",
    "    df.to_csv(tab_file_pred, sep=\",\", header=True, index=False, index_label=False)\n",
    "\n",
    "    # Prediction error\n",
    "    error_pred = df[\"ndefor_pred_ha\"] - df[\"ndefor_obs_ha\"]\n",
    "\n",
    "    # Compute RMSE\n",
    "    squared_error = (error_pred) ** 2\n",
    "    RMSE = round(np.sqrt(np.mean(squared_error)), 2)\n",
    "\n",
    "    # Compute wRMSE\n",
    "    w = df[\"nfor_obs_ha\"] / df[\"nfor_obs_ha\"].sum()\n",
    "    wRMSE = round(np.sqrt(sum(squared_error * w)), 2)\n",
    "\n",
    "    # Compute MedAE\n",
    "    MedAE = round(np.median(np.absolute(error_pred)), 2)\n",
    "\n",
    "    # Calculate R square\n",
    "    # Get the correlation coefficient\n",
    "    r = np.corrcoef(df[\"ndefor_pred_ha\"], df[\"ndefor_obs_ha\"])[0, 1]\n",
    "    # Square the correlation coefficient\n",
    "    r_square = round(r**2, 2)\n",
    "\n",
    "    # Identify model from file\n",
    "    model_basename = os.path.basename(riskmap_file)\n",
    "    model_name = model_basename[5:-7]\n",
    "\n",
    "    # Plot title\n",
    "    title = (\n",
    "        \"{0} model, {1} period\\n\"\n",
    "        \"Predicted vs. observed deforestation \"\n",
    "        \"in {2} ha grid cells.\"\n",
    "    )\n",
    "    title = title.format(model_name, period, csize_coarse_grid_ha)\n",
    "\n",
    "    # Points for identity line\n",
    "    p = [\n",
    "        df[[\"ndefor_obs_ha\", \"ndefor_pred_ha\"]].min(axis=None),\n",
    "        df[[\"ndefor_obs_ha\", \"ndefor_pred_ha\"]].max(axis=None),\n",
    "    ]\n",
    "\n",
    "    # Plot predictions vs. observations\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_box_aspect(1)\n",
    "    plt.scatter(\n",
    "        df[\"ndefor_obs_ha\"], df[\"ndefor_pred_ha\"], color=None, marker=\"o\", edgecolor=\"k\"\n",
    "    )\n",
    "    plt.plot(p, p, \"r--\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Observed deforestation (ha)\")\n",
    "    plt.ylabel(\"Predicted deforestation (ha)\")\n",
    "    # Text indices and ncell\n",
    "    t = \"MedAE = {0:.2f} ha\\nR2 = {1:.2f}\\nn = {2:d}\"\n",
    "    t = t.format(MedAE, r_square, ncell)\n",
    "    x_text = 0\n",
    "    y_text = df[[\"ndefor_obs_ha\", \"ndefor_pred_ha\"]].max(axis=None)\n",
    "    plt.text(x_text, y_text, t, ha=\"left\", va=\"top\")\n",
    "    fig.savefig(fig_file_pred)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Results\n",
    "    indices = {\n",
    "        \"RMSE\": RMSE,\n",
    "        \"wRMSE\": wRMSE,\n",
    "        \"MedAE\": MedAE,\n",
    "        \"R2\": r_square,\n",
    "        \"ncell\": ncell,\n",
    "        \"csize_coarse_grid\": csize_coarse_grid,\n",
    "        \"csize_coarse_grid_ha\": csize_coarse_grid_ha,\n",
    "    }\n",
    "    indices_df = pd.DataFrame([indices])\n",
    "    indices_df.to_csv(\n",
    "        indices_file_pred, sep=\",\", header=True, index=False, index_label=False\n",
    "    )\n",
    "\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"xarray + dask optimized version of `validation_udef_arp` with edge padding.\n",
    "\n",
    "Enhancements:\n",
    "- Uses rioxarray exclusively (assumed installed).\n",
    "- Handles partial tiles at edges by padding arrays before coarsening.\n",
    "- Aggregates data efficiently using Dask-backed xarray operations.\n",
    "\n",
    "This version avoids explicit for-loops, uses block-wise aggregation, and\n",
    "pads edges to ensure all data is included in coarse-grid computations.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray  # assume installed\n",
    "\n",
    "\n",
    "def validation_udef_arp_xr(\n",
    "    fcc_file: str,\n",
    "    time_interval: float,\n",
    "    riskmap_file: str,\n",
    "    tab_file_defor: str,\n",
    "    period: str = \"calibration\",\n",
    "    csize_coarse_grid: int = 300,\n",
    "    indices_file_pred: str = \"indices.csv\",\n",
    "    tab_file_pred: str = \"pred_obs.csv\",\n",
    "    fig_file_pred: str = \"pred_obs.png\",\n",
    "    figsize=(6.4, 6.4),\n",
    "    dpi=100,\n",
    "    verbose: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"xarray/dask optimized validation function with padding edge handling.\"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Read density table\n",
    "    # -------------------------\n",
    "    defor_tab = pd.read_csv(tab_file_defor)\n",
    "    cats = defor_tab[\"cat\"].values\n",
    "    defor_dens = defor_tab[\"defor_dens\"].values\n",
    "    defor_dens_period = defor_dens * time_interval\n",
    "\n",
    "    # -------------------------\n",
    "    # Open rasters with rioxarray + dask chunks\n",
    "    # -------------------------\n",
    "    chunks = {\"x\": csize_coarse_grid, \"y\": csize_coarse_grid}\n",
    "\n",
    "    fcc = rioxarray.open_rasterio(fcc_file, chunks=chunks).squeeze()\n",
    "    risk = rioxarray.open_rasterio(riskmap_file, chunks=chunks).squeeze()\n",
    "\n",
    "    if \"band\" in fcc.dims:\n",
    "        fcc = fcc.isel(band=0)\n",
    "    if \"band\" in risk.dims:\n",
    "        risk = risk.isel(band=0)\n",
    "\n",
    "    # Get pixel area\n",
    "    transform = fcc.rio.transform()\n",
    "    pix_w = abs(transform.a)\n",
    "    pix_h = abs(transform.e)\n",
    "    pix_area = pix_w * pix_h\n",
    "\n",
    "    csize_coarse_grid_ha = round(\n",
    "        csize_coarse_grid * csize_coarse_grid * pix_area / 10000.0, 2\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Create observed forest and deforestation masks\n",
    "    # -------------------------\n",
    "    if period == \"calibration\":\n",
    "        forest_mask = fcc > 0\n",
    "        defor_mask = fcc == 1\n",
    "    elif period == \"validation\":\n",
    "        forest_mask = fcc > 1\n",
    "        defor_mask = fcc == 2\n",
    "    else:  # historical\n",
    "        forest_mask = fcc > 0\n",
    "        defor_mask = xr.apply_ufunc(\n",
    "            lambda a: np.isin(a, [1, 2]),\n",
    "            fcc,\n",
    "            vectorize=True,\n",
    "            dask=\"parallelized\",\n",
    "            output_dtypes=[bool],\n",
    "        )\n",
    "\n",
    "    forest_int = forest_mask.astype(int)\n",
    "    defor_int = defor_mask.astype(int)\n",
    "\n",
    "    # -------------------------\n",
    "    # Pad to make dimensions divisible by csize_coarse_grid\n",
    "    # -------------------------\n",
    "    ny = forest_int.sizes[\"y\"]\n",
    "    nx = forest_int.sizes[\"x\"]\n",
    "    pad_y = (csize_coarse_grid - ny % csize_coarse_grid) % csize_coarse_grid\n",
    "    pad_x = (csize_coarse_grid - nx % csize_coarse_grid) % csize_coarse_grid\n",
    "    # Ensure risk can accept NaNs\n",
    "    if np.issubdtype(risk.dtype, np.integer):\n",
    "        risk = risk.astype(float)\n",
    "    if pad_y > 0 or pad_x > 0:\n",
    "        pad_width = {\"y\": (0, pad_y), \"x\": (0, pad_x)}\n",
    "        forest_int = forest_int.pad(pad_width, mode=\"constant\", constant_values=0)\n",
    "        defor_int = defor_int.pad(pad_width, mode=\"constant\", constant_values=0)\n",
    "        risk = risk.pad(pad_width, mode=\"constant\", constant_values=np.nan)\n",
    "\n",
    "    # -------------------------\n",
    "    # Aggregate using coarsen (safe with divisible dims)\n",
    "    # -------------------------\n",
    "    factor_y = csize_coarse_grid\n",
    "    factor_x = csize_coarse_grid\n",
    "\n",
    "    forest_coarse = forest_int.coarsen(y=factor_y, x=factor_x).sum()\n",
    "    defor_coarse = defor_int.coarsen(y=factor_y, x=factor_x).sum()\n",
    "\n",
    "    # Category counts per coarse cell\n",
    "    cat_counts_list = []\n",
    "    for c in cats:\n",
    "        mask = (risk == c).astype(int)\n",
    "        mask_coarse = mask.coarsen(y=factor_y, x=factor_x).sum()\n",
    "        cat_counts_list.append(mask_coarse)\n",
    "\n",
    "    cat_counts = xr.concat(cat_counts_list, dim=\"cat\").assign_coords(cat=(\"cat\", cats))\n",
    "\n",
    "    dens_da = xr.DataArray(defor_dens_period, coords={\"cat\": cats}, dims=(\"cat\",))\n",
    "    pred_ha_per_cat = cat_counts * dens_da\n",
    "    pred_ha_coarse = pred_ha_per_cat.sum(dim=\"cat\")\n",
    "\n",
    "    forest_ha_coarse = forest_coarse * (pix_area / 10000.0)\n",
    "    defor_ha_coarse = defor_coarse * (pix_area / 10000.0)\n",
    "\n",
    "    # -------------------------\n",
    "    # Compute results (trigger computation)\n",
    "    # -------------------------\n",
    "    if verbose:\n",
    "        print(\"Computing aggregated arrays with padding...\")\n",
    "\n",
    "    forest_ha = forest_ha_coarse.compute()\n",
    "    defor_ha_obs = defor_ha_coarse.compute()\n",
    "    pred_ha = pred_ha_coarse.compute()\n",
    "\n",
    "    df = (\n",
    "        xr.Dataset(\n",
    "            {\n",
    "                \"nfor_obs_ha\": forest_ha,\n",
    "                \"ndefor_obs_ha\": defor_ha_obs,\n",
    "                \"ndefor_pred_ha\": pred_ha,\n",
    "            }\n",
    "        )\n",
    "        .to_dataframe()\n",
    "        .reset_index()\n",
    "        .dropna()\n",
    "    )\n",
    "\n",
    "    df = df[df[\"nfor_obs_ha\"] > 0]\n",
    "    ncell = df.shape[0]\n",
    "\n",
    "    df[\"nfor_obs\"] = (df[\"nfor_obs_ha\"] * 10000.0 / pix_area).round().astype(int)\n",
    "    df[\"ndefor_obs\"] = (df[\"ndefor_obs_ha\"] * 10000.0 / pix_area).round().astype(int)\n",
    "\n",
    "    df_out = df[\n",
    "        [\n",
    "            \"y\",\n",
    "            \"x\",\n",
    "            \"nfor_obs\",\n",
    "            \"ndefor_obs\",\n",
    "            \"nfor_obs_ha\",\n",
    "            \"ndefor_obs_ha\",\n",
    "            \"ndefor_pred_ha\",\n",
    "        ]\n",
    "    ]\n",
    "    df_out.to_csv(tab_file_pred, index=False)\n",
    "\n",
    "    # -------------------------\n",
    "    # Compute metrics\n",
    "    # -------------------------\n",
    "    error_pred = df_out[\"ndefor_pred_ha\"] - df_out[\"ndefor_obs_ha\"]\n",
    "    squared_error = error_pred**2\n",
    "    RMSE = round(np.sqrt(np.mean(squared_error)), 2)\n",
    "\n",
    "    w = df_out[\"nfor_obs_ha\"] / df_out[\"nfor_obs_ha\"].sum()\n",
    "    wRMSE = round(np.sqrt(np.sum(squared_error * w)), 2)\n",
    "    MedAE = round(np.median(np.abs(error_pred)), 2)\n",
    "\n",
    "    if ncell > 1:\n",
    "        r = np.corrcoef(df_out[\"ndefor_pred_ha\"], df_out[\"ndefor_obs_ha\"])[0, 1]\n",
    "        r_square = round(float(r**2), 2)\n",
    "    else:\n",
    "        r_square = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Plot\n",
    "    # -------------------------\n",
    "    title = f\"Predicted vs Observed Deforestation ({period}) in {csize_coarse_grid_ha} ha cells\"\n",
    "    pmin = min(df_out[[\"ndefor_obs_ha\", \"ndefor_pred_ha\"]].min().min(), 0)\n",
    "    pmax = df_out[[\"ndefor_obs_ha\", \"ndefor_pred_ha\"]].max().max()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    ax.scatter(\n",
    "        df_out[\"ndefor_obs_ha\"], df_out[\"ndefor_pred_ha\"], marker=\"o\", edgecolor=\"k\"\n",
    "    )\n",
    "    ax.plot([pmin, pmax], [pmin, pmax], \"r--\")\n",
    "    ax.set_xlabel(\"Observed deforestation (ha)\")\n",
    "    ax.set_ylabel(\"Predicted deforestation (ha)\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    t = f\"MedAE = {MedAE:.2f} ha\\nR2 = {r_square}\\nn = {ncell:d}\"\n",
    "    ax.text(0, pmax, t, ha=\"left\", va=\"top\")\n",
    "\n",
    "    fig.savefig(fig_file_pred)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # -------------------------\n",
    "    # Save indices\n",
    "    # -------------------------\n",
    "    indices = {\n",
    "        \"RMSE\": RMSE,\n",
    "        \"wRMSE\": wRMSE,\n",
    "        \"MedAE\": MedAE,\n",
    "        \"R2\": r_square,\n",
    "        \"ncell\": int(ncell),\n",
    "        \"csize_coarse_grid\": csize_coarse_grid,\n",
    "        \"csize_coarse_grid_ha\": csize_coarse_grid_ha,\n",
    "    }\n",
    "    pd.DataFrame([indices]).to_csv(indices_file_pred, index=False)\n",
    "\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a11e2-5dd4-4444-8e5c-3306857ca6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forestatrisk as far\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "    fcc_file,\n",
    "    csizes_val,\n",
    "    val_periods,\n",
    "    val_models,\n",
    "    available_prediction_files,\n",
    "    available_defrate_files,\n",
    "    period_dict,\n",
    "):\n",
    "    for csize_val in csizes_val:\n",
    "        for period in val_periods:\n",
    "            period_output_folder = evaluation_folder / period\n",
    "            period_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "            for model in val_models:\n",
    "                riskmap_file = filter_files(\n",
    "                    available_prediction_files, [model, period], None, True\n",
    "                )[0]\n",
    "                defrate_file = filter_files(\n",
    "                    available_defrate_files, [model, period], None, True\n",
    "                )[0]\n",
    "                far.validation_udef_arp(\n",
    "                    # validation_udef_arp_xr(\n",
    "                    fcc_file=fcc_file,\n",
    "                    period=period,\n",
    "                    time_interval=period_dict[period][\"time_interval\"],\n",
    "                    riskmap_file=riskmap_file,\n",
    "                    tab_file_defor=defrate_file,\n",
    "                    csize_coarse_grid=csize_val,\n",
    "                    indices_file_pred=period_output_folder\n",
    "                    / f\"indices_{model}_{period}_{csize_val}.csv\",\n",
    "                    tab_file_pred=period_output_folder\n",
    "                    / f\"pred_obs_{model}_{period}_{csize_val}.csv\",\n",
    "                    fig_file_pred=period_output_folder\n",
    "                    / f\"pred_obs_{model}_{period}_{csize_val}.png\",\n",
    "                    verbose=False,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d97fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forestatrisk as far\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "    fcc_file: Path,\n",
    "    csizes_val: list[int],\n",
    "    models_dict: Dict,\n",
    "    period_dict: Dict,\n",
    "):\n",
    "    for csize_val in csizes_val:\n",
    "        for key, value in models_dict.items():\n",
    "            identifier, subsystem, period = key\n",
    "            riskmap_file, defrate_file = value\n",
    "            period_output_folder = evaluation_folder / period\n",
    "            period_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "            far.validation_udef_arp(\n",
    "                # validation_udef_arp_xr(\n",
    "                fcc_file=fcc_file,\n",
    "                period=period,\n",
    "                time_interval=period_dict[period][\"time_interval\"],\n",
    "                riskmap_file=riskmap_file,\n",
    "                tab_file_defor=defrate_file,\n",
    "                csize_coarse_grid=csize_val,\n",
    "                indices_file_pred=period_output_folder\n",
    "                / f\"indices_{identifier}_{csize_val}.csv\",\n",
    "                tab_file_pred=period_output_folder\n",
    "                / f\"pred_obs_{identifier}_{csize_val}.csv\",\n",
    "                fig_file_pred=period_output_folder\n",
    "                / f\"pred_obs_{identifier}_{csize_val}.png\",\n",
    "                verbose=False,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea416463-3912-4c86-b864-684def3d708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(\n",
    "    forest_change_file,\n",
    "    coarse_grid_cell_size_pixels,\n",
    "    models_dict,\n",
    "    period_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034814f2-42b3-4a5d-8800-f8fe431919e2",
   "metadata": {},
   "source": [
    "## Join all the indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4854c-908f-4638-a582-c4a0595c9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_csv_files = list_files_by_extension(evaluation_folder, [\".csv\"], True)\n",
    "indices_csv_files = filter_files(evaluation_csv_files, [\"indices\"], None, False)\n",
    "indices_csv_files_clean = filter_out_ipynb_checkpoints(indices_csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971ec78-fb83-4af1-95b7-a382de8ed83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def extract_info_from_filename(filepath):\n",
    "    \"\"\"\n",
    "    Extracts period and model from a given filename.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The full path to the file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (period, model).\n",
    "    \"\"\"\n",
    "    # Convert the filepath to a Path object\n",
    "    path = Path(filepath)\n",
    "\n",
    "    # Get the filename without the extension\n",
    "    filename = path.stem\n",
    "\n",
    "    # Split the filename by underscores\n",
    "    parts = filename.split(\"_\")\n",
    "\n",
    "    # The period is always the last part before the number (which is the last part)\n",
    "    # Find where the numeric part starts\n",
    "    for i in range(len(parts) - 1, 0, -1):\n",
    "        if parts[i].isdigit():\n",
    "            # The model is between 'indices' and the period\n",
    "            period = parts[i - 1]\n",
    "            # The model name can contain underscores\n",
    "            model_parts = parts[\n",
    "                1 : i - 1\n",
    "            ]  # Skip 'indices' (index 0) and period (index i-1)\n",
    "            model = \"_\".join(model_parts)\n",
    "            return period, model\n",
    "\n",
    "    # If no numeric part is found, fallback to original logic\n",
    "    period = parts[-1]\n",
    "    model = parts[-2]\n",
    "    return period, model\n",
    "\n",
    "\n",
    "def combine_model_results(indices_files_list):\n",
    "    \"\"\"Combine model results for comparison.\"\"\"\n",
    "    indices_list = []\n",
    "    for file in indices_files_list:\n",
    "        if Path(file).is_file():\n",
    "            period, model = extract_info_from_filename(file)\n",
    "            df = pd.read_csv(file)\n",
    "            df[\"model\"] = model\n",
    "            df[\"period\"] = period\n",
    "            indices_list.append(df)\n",
    "        # Concat indices\n",
    "        indices = pd.concat(indices_list, axis=0)\n",
    "        indices.sort_values(by=[\"csize_coarse_grid\", \"period\", \"model\"])\n",
    "        indices = indices[\n",
    "            [\n",
    "                \"csize_coarse_grid\",\n",
    "                \"csize_coarse_grid_ha\",\n",
    "                \"ncell\",\n",
    "                \"period\",\n",
    "                \"model\",\n",
    "                \"MedAE\",\n",
    "                \"R2\",\n",
    "                \"RMSE\",\n",
    "                \"wRMSE\",\n",
    "            ]\n",
    "        ]\n",
    "    indices.to_csv(\n",
    "        os.path.join(evaluation_folder, \"indices_all.csv\"),\n",
    "        sep=\",\",\n",
    "        header=True,\n",
    "        index=False,\n",
    "        index_label=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d426ee-aec7-47e1-98fb-e5da25c4abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_model_results(indices_csv_files_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6512b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deforisk-jupyter-nb (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
