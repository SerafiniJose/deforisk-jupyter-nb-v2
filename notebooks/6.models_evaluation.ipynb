{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93956c07-d9f1-4e3a-ae1b-148aec301334",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "# import pandas as pd\n",
    "# print(pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100697fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizations\n",
    "# GDAL optimizations\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "cpu_count: int = mp.cpu_count()\n",
    "num_cores: int = cpu_count - 2\n",
    "os.environ[\"GDAL_NUM_THREADS\"] = f\"{num_cores}\"\n",
    "os.environ[\"GDAL_CACHEMAX\"] = \"1024\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2a25b-11a9-436c-9eff-17ad4003f00c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13a12a-7502-4c76-b490-9880fbf99c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import riskmapjnr as rmj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15850cf5-6362-4e08-aef7-f9372ff90576",
   "metadata": {},
   "source": [
    "## Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a76eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2020, 2024]\n",
    "tree_cover_threshold = 10\n",
    "forest_source = \"gfc\"  ##gfc, tmf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc668b-a41e-4bbe-9942-39610df8a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_grid_cell_size_pixels: list[int] = [300]\n",
    "models_to_compare: list[str] = [\"rmj_bm\", \"rmj_mw\", \"far_rf\", \"far_icar\"]\n",
    "periods: list[str] = [\"calibration\", \"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c312be-754e-43c3-b465-40c41963b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coarse_grid_cell_size_pixels = [50, 100]\n",
    "# models_to_compare = [\"rmj_bm\", \"rmj_mw\", \"rf\", \"icar\", \"glm\", \"user\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d8fcd-6d62-4b47-a589-5921a4cc8102",
   "metadata": {},
   "source": [
    "## Connect folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder: Path = Path.cwd().parent\n",
    "downloads_folder: Path = root_folder / \"data\"\n",
    "downloads_folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f775e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = downloads_folder / project_name\n",
    "project_folder.mkdir(parents=True, exist_ok=True)\n",
    "processed_data_folder = project_folder / \"data\"\n",
    "processed_data_folder.mkdir(parents=True, exist_ok=True)\n",
    "evaluation_folder = project_folder / \"evaluation\"\n",
    "evaluation_folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff58466-9b05-4d17-b733-bcd2c523a01d",
   "metadata": {},
   "source": [
    "## Select predictions files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99809c-f245-40c8-a058-e0b39b7ffb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders(directory):\n",
    "    \"\"\"\n",
    "    Lists all folders (directories) within a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): The path to the directory from which to list folders.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of folder names within the specified directory.\n",
    "              If an error occurs, returns an empty list and prints an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a Path object for the directory\n",
    "        path = Path(directory)\n",
    "\n",
    "        # Filter out only directories (folders) using is_dir()\n",
    "        folders = [entry for entry in path.iterdir() if entry.is_dir()]\n",
    "\n",
    "        return folders\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac38001-5c03-4f0c-bb35-74f53050b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_folders(input_folders, filter_words, exclude_words=None):\n",
    "    \"\"\"\n",
    "    Filters a list of folders based on include and exclude words.\n",
    "    Parameters:\n",
    "        input_folders (list): List of folder names to be filtered.\n",
    "        filter_words (list): Words that must be present in the folder names for inclusion.\n",
    "        exclude_words (list, optional): Words that must not be present in the folder names for exclusion. Defaults to None.\n",
    "    Returns:\n",
    "        list: Filtered list of folders.\n",
    "    \"\"\"\n",
    "    # Ensure all words are lowercase for case-insensitive comparison\n",
    "    filter_words = [word.lower() for word in filter_words]\n",
    "    exclude_words = [word.lower() for word in (exclude_words or [])]\n",
    "\n",
    "    filtered_folders = [\n",
    "        folder\n",
    "        for folder in input_folders\n",
    "        if any(word in folder.name.lower() for word in filter_words)\n",
    "        and not any(\n",
    "            exclude_word in folder.name.lower() for exclude_word in exclude_words\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return filtered_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e38df2-acf5-4572-a2cc-8a02782be7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_by_extension(folder_path, file_extensions, recursive=False):\n",
    "    \"\"\"\n",
    "    List all files with specified extensions in the given folder.\n",
    "    Parameters:\n",
    "    folder_path (str or Path): The path to the folder where you want to search for files.\n",
    "    file_extensions (list of str): A list of file extensions to search for (e.g., ['.shp', '.tif']).\n",
    "    recursive (bool): Whether to recursively search through subdirectories or not.\n",
    "    Returns:\n",
    "    list: A list of file paths with the specified extensions.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "    try:\n",
    "        # Convert folder_path to Path object if it's a string\n",
    "        folder_path = Path(folder_path)\n",
    "\n",
    "        # Check if the provided path is a directory\n",
    "        if folder_path.is_dir():\n",
    "            for entry in folder_path.iterdir():\n",
    "                if entry.is_file() and any(\n",
    "                    entry.suffix.lower() == ext.lower() for ext in file_extensions\n",
    "                ):\n",
    "                    matching_files.append(str(entry))\n",
    "                elif recursive and entry.is_dir():\n",
    "                    # Recursively search subdirectories\n",
    "                    matching_files.extend(\n",
    "                        list_files_by_extension(entry, file_extensions, recursive)\n",
    "                    )\n",
    "        else:\n",
    "            print(f\"The provided path '{folder_path}' is not a directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return matching_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6104aeb-e4cf-45a0-99dd-2d6e248daee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files(input_files, filter_words, exclude_words=None, include_all=True):\n",
    "    \"\"\"\n",
    "    Filters a list of files based on include and exclude words.\n",
    "    Parameters:\n",
    "        input_files (list): List of file paths to be filtered.\n",
    "        filter_words (list): Words that must be present in the filenames for inclusion.\n",
    "        exclude_words (list, optional): Words that must not be present in the filenames for exclusion. Defaults to None.\n",
    "        include_all (bool, optional): If True, all filter words must be present in the filename. If False, at least one of the filter words must be present. Defaults to False.\n",
    "    Returns:\n",
    "        list: Filtered list of files.\n",
    "    \"\"\"\n",
    "    # Ensure all words are lowercase for case-insensitive comparison\n",
    "    filter_words = [word.lower() for word in filter_words]\n",
    "    exclude_words = [word.lower() for word in (exclude_words or [])]\n",
    "\n",
    "    if include_all:\n",
    "        filtered_files = [\n",
    "            file\n",
    "            for file in input_files\n",
    "            if all(word in Path(file).name.lower() for word in filter_words)\n",
    "            and not any(\n",
    "                exclude_word in Path(file).name.lower()\n",
    "                for exclude_word in exclude_words\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        filtered_files = [\n",
    "            file\n",
    "            for file in input_files\n",
    "            if any(word in Path(file).name.lower() for word in filter_words)\n",
    "            and not any(\n",
    "                exclude_word in Path(file).name.lower()\n",
    "                for exclude_word in exclude_words\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return filtered_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7e6b7-a73b-4b6d-a699-576996f7c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_ipynb_checkpoints(input_files):\n",
    "    \"\"\"\n",
    "    Filters out files whose paths contain '.ipynb_checkpoints'.\n",
    "    Parameters:\n",
    "        input_files (list): List of file paths to be filtered.\n",
    "    Returns:\n",
    "        list: Filtered list of files.\n",
    "    \"\"\"\n",
    "    filtered_files = [\n",
    "        file for file in input_files if \".ipynb_checkpoints\" not in Path(file).parts\n",
    "    ]\n",
    "    filtered_files = [\n",
    "        file for file in input_files if \"indices_all\" not in Path(file).parts\n",
    "    ]\n",
    "    return filtered_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd71cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = project_folder\n",
    "folders = list_folders(directory_path)\n",
    "available_models = filter_folders(folders, models_to_compare, [\"data\", \"data_raw\"])\n",
    "print(\"Models_available:\", available_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c67c32-6e2f-4a17-89ef-d0182fa46122",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = list_folders(project_folder)\n",
    "available_models = filter_folders(folders, models_to_compare, [\"data\", \"data_raw\"])\n",
    "\n",
    "available_prediction_files = []\n",
    "for model_folder in available_models:\n",
    "    tif_files = list_files_by_extension(model_folder, [\".tif\"], True)\n",
    "    model_files = filter_files(tif_files, periods, None, False)\n",
    "    available_prediction_files.append(model_files)\n",
    "\n",
    "available_defrate_files = []\n",
    "for model_folder in available_models:\n",
    "    csv_files = list_files_by_extension(model_folder, [\".csv\"], True)\n",
    "    defrate_files = filter_files(csv_files, periods, None, False)\n",
    "    defrate_files1 = filter_files(defrate_files, [\"defrate\"])\n",
    "    defrate_files2 = filter_out_ipynb_checkpoints(defrate_files1)\n",
    "    available_defrate_files.append(defrate_files2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d7084-ac1a-4521-97c6-c84b13d7422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_prediction_files = sum(available_prediction_files, [])\n",
    "available_defrate_files = sum(available_defrate_files, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730627c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries mapping common names to paths\n",
    "dict1 = {}\n",
    "dict2 = {}\n",
    "\n",
    "\n",
    "# Function to extract the subsystem/model part from path\n",
    "def extract_subsystem(path):\n",
    "    \"\"\"Extract the subsystem identifier from path structure\"\"\"\n",
    "    path_obj = Path(path)\n",
    "\n",
    "    # Get the path components and look for key directories like rmj_bm, rmj_mw, etc.\n",
    "    # The pattern is: .../test/{subsystem}/{validation|calibration}/...\n",
    "    path_parts = path_obj.parts\n",
    "\n",
    "    # Find the subsystem by looking for test directory and its immediate subdirectory\n",
    "    try:\n",
    "        test_index = path_parts.index(\"test\")\n",
    "        if test_index + 1 < len(path_parts):\n",
    "            return path_parts[\n",
    "                test_index + 1\n",
    "            ]  # This should be the subsystem like rmj_bm\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to extract the period (validation/calibration/historical) from path\n",
    "def extract_period(path):\n",
    "    \"\"\"Extract the period identifier from path structure\"\"\"\n",
    "    path_obj = Path(path)\n",
    "\n",
    "    # Get the path components\n",
    "    path_parts = path_obj.parts\n",
    "\n",
    "    # Find the period by looking for validation or calibration directories\n",
    "    try:\n",
    "        for i, part in enumerate(path_parts):\n",
    "            if part in [\"validation\", \"calibration\", \"historical\"]:\n",
    "                return part\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "for path in available_prediction_files:\n",
    "    path_obj = Path(path)\n",
    "    filename = path_obj.name\n",
    "    subsystem = extract_subsystem(path)\n",
    "    period = extract_period(path)\n",
    "\n",
    "    name_no_ext = Path(filename).stem\n",
    "    if name_no_ext.startswith(\"prob_\"):\n",
    "        identifier = name_no_ext[len(\"prob_\") :]\n",
    "    else:\n",
    "        identifier: str = name_no_ext\n",
    "    key = (identifier, subsystem, period)\n",
    "    if key not in dict1:\n",
    "        dict1[key] = []\n",
    "    dict1[key].append(path)\n",
    "\n",
    "for path in available_defrate_files:\n",
    "    path_obj = Path(path)\n",
    "    filename = path_obj.name\n",
    "    subsystem = extract_subsystem(path)\n",
    "    period = extract_period(path)\n",
    "\n",
    "    name_no_ext = Path(filename).stem\n",
    "    identifier = name_no_ext[len(\"defrate_cat_\") :]\n",
    "\n",
    "    key = (identifier, subsystem, period)\n",
    "    if key not in dict2:\n",
    "        dict2[key] = []\n",
    "    dict2[key].append(path)\n",
    "\n",
    "\n",
    "# Create the final matching dictionary - now we match by both identifier and subsystem\n",
    "models_dict = {}\n",
    "for key1 in dict1:\n",
    "    if key1 in dict2:\n",
    "        # Create all possible combinations between tiff and csv files with same identifier and subsystem\n",
    "        identifier, subsystem, period = key1\n",
    "        for tiff_path in dict1[key1]:\n",
    "            for csv_path in dict2[key1]:\n",
    "                models_dict[(identifier, subsystem, period)] = (\n",
    "                    tiff_path,\n",
    "                    csv_path,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final matching dictionary with all attributes:\")\n",
    "for key, value in models_dict.items():\n",
    "    identifier, subsystem, period = key\n",
    "    tiff_path, csv_path = value\n",
    "    print(f\"Identifier '{identifier}', Subsystem '{subsystem}', Period '{period}':\")\n",
    "    print(f\"  TIFF: {tiff_path}\")\n",
    "    print(f\"  CSV:  {csv_path}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba919a-3341-442e-924d-ee870327731d",
   "metadata": {},
   "source": [
    "## Select forest cover change file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d17f5-7b11-481e-8cde-a49d0b51d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all raster files in the processed data folder\n",
    "input_raster_files = list_files_by_extension(processed_data_folder, [\".tiff\", \".tif\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9d584-29bd-436a-a7f1-aa3a9a0e030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_change_file = filter_files(\n",
    "    input_raster_files,\n",
    "    [\"forest\", \"loss\", forest_source] + [str(num) for num in years],\n",
    "    [\"distance\", \"edge\"],\n",
    ")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dad12b-f99b-4c5e-a77c-0af59155e580",
   "metadata": {},
   "source": [
    "## Periods dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be2707-82be-4def-be45-ca8a61ce8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_dict = {\n",
    "    \"period\": \"calibration\",\n",
    "    \"initial_year\": years[0],\n",
    "    \"final_year\": years[1],\n",
    "    \"defor_value\": 1,\n",
    "    \"time_interval\": years[1] - years[0],\n",
    "}\n",
    "validation_dict = {\n",
    "    \"period\": \"validation\",\n",
    "    \"initial_year\": years[1],\n",
    "    \"final_year\": years[2],\n",
    "    \"defor_value\": 1,\n",
    "    \"time_interval\": years[2] - years[1],\n",
    "}\n",
    "historical_dict = {\n",
    "    \"period\": \"historical\",\n",
    "    \"initial_year\": years[0],\n",
    "    \"final_year\": years[2],\n",
    "    \"defor_value\": [1, 2],\n",
    "    \"time_interval\": years[2] - years[0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77fdb0-3ae6-4b22-92e3-f5ddb76869be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el diccionario principal\n",
    "period_dict = {\n",
    "    calibration_dict[\"period\"]: calibration_dict,\n",
    "    validation_dict[\"period\"]: validation_dict,\n",
    "    historical_dict[\"period\"]: historical_dict,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c886152-5c18-4daf-884b-45434df2b29a",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a11e2-5dd4-4444-8e5c-3306857ca6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forestatrisk as far\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "    fcc_file,\n",
    "    csizes_val,\n",
    "    val_periods,\n",
    "    val_models,\n",
    "    available_prediction_files,\n",
    "    available_defrate_files,\n",
    "    period_dict,\n",
    "):\n",
    "    for csize_val in csizes_val:\n",
    "        for period in val_periods:\n",
    "            period_output_folder = evaluation_folder / period\n",
    "            period_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "            for model in val_models:\n",
    "                riskmap_file = filter_files(\n",
    "                    available_prediction_files, [model, period], None, True\n",
    "                )[0]\n",
    "                defrate_file = filter_files(\n",
    "                    available_defrate_files, [model, period], None, True\n",
    "                )[0]\n",
    "                far.validation_udef_arp(\n",
    "                    # validation_udef_arp_xr(\n",
    "                    fcc_file=fcc_file,\n",
    "                    period=period,\n",
    "                    time_interval=period_dict[period][\"time_interval\"],\n",
    "                    riskmap_file=riskmap_file,\n",
    "                    tab_file_defor=defrate_file,\n",
    "                    csize_coarse_grid=csize_val,\n",
    "                    indices_file_pred=period_output_folder\n",
    "                    / f\"indices_{model}_{period}_{csize_val}.csv\",\n",
    "                    tab_file_pred=period_output_folder\n",
    "                    / f\"pred_obs_{model}_{period}_{csize_val}.csv\",\n",
    "                    fig_file_pred=period_output_folder\n",
    "                    / f\"pred_obs_{model}_{period}_{csize_val}.png\",\n",
    "                    verbose=False,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d97fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forestatrisk as far\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "    fcc_file: Path,\n",
    "    csizes_val: list[int],\n",
    "    models_dict: Dict,\n",
    "    period_dict: Dict,\n",
    "):\n",
    "    for csize_val in csizes_val:\n",
    "        for key, value in models_dict.items():\n",
    "            identifier, subsystem, period = key\n",
    "            riskmap_file, defrate_file = value\n",
    "            period_output_folder = evaluation_folder / period\n",
    "            period_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "            if not Path(\n",
    "                period_output_folder / f\"indices_{identifier}_{csize_val}.csv\"\n",
    "            ).exists():\n",
    "                far.validation_udef_arp(\n",
    "                    fcc_file=fcc_file,\n",
    "                    period=period,\n",
    "                    time_interval=period_dict[period][\"time_interval\"],\n",
    "                    riskmap_file=riskmap_file,\n",
    "                    tab_file_defor=defrate_file,\n",
    "                    csize_coarse_grid=csize_val,\n",
    "                    indices_file_pred=period_output_folder\n",
    "                    / f\"indices_{identifier}_{csize_val}.csv\",\n",
    "                    tab_file_pred=period_output_folder\n",
    "                    / f\"pred_obs_{identifier}_{csize_val}.csv\",\n",
    "                    fig_file_pred=period_output_folder\n",
    "                    / f\"pred_obs_{identifier}_{csize_val}.png\",\n",
    "                    verbose=False,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea416463-3912-4c86-b864-684def3d708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(\n",
    "    forest_change_file,\n",
    "    coarse_grid_cell_size_pixels,\n",
    "    models_dict,\n",
    "    period_dict,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034814f2-42b3-4a5d-8800-f8fe431919e2",
   "metadata": {},
   "source": [
    "## Join all the indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4854c-908f-4638-a582-c4a0595c9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_csv_files = list_files_by_extension(evaluation_folder, [\".csv\"], True)\n",
    "indices_csv_files = filter_files(evaluation_csv_files, [\"indices\"], None, False)\n",
    "indices_csv_files_clean = filter_out_ipynb_checkpoints(indices_csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971ec78-fb83-4af1-95b7-a382de8ed83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def extract_info_from_filename(filepath):\n",
    "    \"\"\"\n",
    "    Extracts period and model from a given filename.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The full path to the file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (period, model).\n",
    "    \"\"\"\n",
    "    # Convert the filepath to a Path object\n",
    "    path = Path(filepath)\n",
    "\n",
    "    # Get the filename without the extension\n",
    "    filename = path.stem\n",
    "\n",
    "    # Split the filename by underscores\n",
    "    parts = filename.split(\"_\")\n",
    "\n",
    "    # The period is always the last part before the number (which is the last part)\n",
    "    # Find where the numeric part starts\n",
    "    for i in range(len(parts) - 1, 0, -1):\n",
    "        if parts[i].isdigit():\n",
    "            # The model is between 'indices' and the period\n",
    "            period = parts[i - 1]\n",
    "            # The model name can contain underscores\n",
    "            model_parts = parts[\n",
    "                1 : i - 1\n",
    "            ]  # Skip 'indices' (index 0) and period (index i-1)\n",
    "            model = \"_\".join(model_parts)\n",
    "            return period, model\n",
    "\n",
    "    # If no numeric part is found, fallback to original logic\n",
    "    period = parts[-1]\n",
    "    model = parts[-2]\n",
    "    return period, model\n",
    "\n",
    "\n",
    "def combine_model_results(indices_files_list):\n",
    "    \"\"\"Combine model results for comparison.\"\"\"\n",
    "    indices_list = []\n",
    "    for file in indices_files_list:\n",
    "        if Path(file).is_file():\n",
    "            period, model = extract_info_from_filename(file)\n",
    "            df = pd.read_csv(file)\n",
    "            df[\"model\"] = model\n",
    "            df[\"period\"] = period\n",
    "            indices_list.append(df)\n",
    "        # Concat indices\n",
    "        indices = pd.concat(indices_list, axis=0)\n",
    "        indices.sort_values(by=[\"csize_coarse_grid\", \"period\", \"model\"])\n",
    "        indices = indices[\n",
    "            [\n",
    "                \"csize_coarse_grid\",\n",
    "                \"csize_coarse_grid_ha\",\n",
    "                \"ncell\",\n",
    "                \"period\",\n",
    "                \"model\",\n",
    "                \"MedAE\",\n",
    "                \"R2\",\n",
    "                \"RMSE\",\n",
    "                \"wRMSE\",\n",
    "            ]\n",
    "        ]\n",
    "    indices.to_csv(\n",
    "        os.path.join(evaluation_folder, \"indices_all.csv\"),\n",
    "        sep=\",\",\n",
    "        header=True,\n",
    "        index=False,\n",
    "        index_label=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d426ee-aec7-47e1-98fb-e5da25c4abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_model_results(indices_csv_files_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6512b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deforisk-jupyter-nb (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
