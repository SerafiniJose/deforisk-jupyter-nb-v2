{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93956c07-d9f1-4e3a-ae1b-148aec301334",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2a25b-11a9-436c-9eff-17ad4003f00c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13a12a-7502-4c76-b490-9880fbf99c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import pkg_resources\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "import riskmapjnr as rmj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46917d46-f655-47a2-b1ec-e24c20bc1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDAL\n",
    "os.environ[\"GDAL_CACHEMAX\"] = \"1024\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15850cf5-6362-4e08-aef7-f9372ff90576",
   "metadata": {},
   "source": [
    "## Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106736a8-74db-4767-b3c2-f7c398b97aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_code = \"MTQ\"\n",
    "years = [2000, 2010, 2023]\n",
    "forest_source = \"tmf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c312be-754e-43c3-b465-40c41963b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = [\"rmj_bm\", \"rmj_mw\", \"rf\", \"icar\", \"glm\", \"user\"]\n",
    "model_to_allocate = [\"rmj_bm\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a1867a-2840-4590-9103-3fed5764cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jurisdiction_expected_total_deforestation_ha = 1000\n",
    "years_to_forecast = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb09fa8-2404-401f-a5e5-6d7ad8bf002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [\"forecast\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d8fcd-6d62-4b47-a589-5921a4cc8102",
   "metadata": {},
   "source": [
    "## Connect folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d607a-c906-459f-8039-3a1bb1abd73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = pathlib.Path.cwd()\n",
    "downloads_folder = root_folder / \"downloads\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07074b-2844-460a-98ca-e8c8917c9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = downloads_folder / iso_code\n",
    "processed_data_folder = project_folder / \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a0f0b-4aba-44d0-a391-746ea19ca8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "allocation_folder = project_folder / \"allocation\"\n",
    "if not os.path.exists(allocation_folder):\n",
    "    os.makedirs(allocation_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff58466-9b05-4d17-b733-bcd2c523a01d",
   "metadata": {},
   "source": [
    "## Select predictions files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99809c-f245-40c8-a058-e0b39b7ffb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders(directory):\n",
    "    \"\"\"\n",
    "    Lists all folders (directories) within a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): The path to the directory from which to list folders.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of folder names within the specified directory.\n",
    "              If an error occurs, returns an empty list and prints an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a Path object for the directory\n",
    "        path = pathlib.Path(directory)\n",
    "\n",
    "        # Filter out only directories (folders) using is_dir()\n",
    "        folders = [entry for entry in path.iterdir() if entry.is_dir()]\n",
    "\n",
    "        return folders\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac38001-5c03-4f0c-bb35-74f53050b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_folders(input_folders, filter_words, exclude_words=None):\n",
    "    \"\"\"\n",
    "    Filters a list of folders based on include and exclude words.\n",
    "    Parameters:\n",
    "        input_folders (list): List of folder names to be filtered.\n",
    "        filter_words (list): Words that must be present in the folder names for inclusion.\n",
    "        exclude_words (list, optional): Words that must not be present in the folder names for exclusion. Defaults to None.\n",
    "    Returns:\n",
    "        list: Filtered list of folders.\n",
    "    \"\"\"\n",
    "    # Ensure all words are lowercase for case-insensitive comparison\n",
    "    filter_words = [word.lower() for word in filter_words]\n",
    "    exclude_words = [word.lower() for word in (exclude_words or [])]\n",
    "\n",
    "    filtered_folders = [\n",
    "        folder\n",
    "        for folder in input_folders\n",
    "        if any(word in folder.name.lower() for word in filter_words)\n",
    "        and not any(\n",
    "            exclude_word in folder.name.lower() for exclude_word in exclude_words\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return filtered_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e38df2-acf5-4572-a2cc-8a02782be7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_by_extension(folder_path, file_extensions, recursive=False):\n",
    "    \"\"\"\n",
    "    List all files with specified extensions in the given folder.\n",
    "    Parameters:\n",
    "    folder_path (str or Path): The path to the folder where you want to search for files.\n",
    "    file_extensions (list of str): A list of file extensions to search for (e.g., ['.shp', '.tif']).\n",
    "    recursive (bool): Whether to recursively search through subdirectories or not.\n",
    "    Returns:\n",
    "    list: A list of file paths with the specified extensions.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "    try:\n",
    "        # Convert folder_path to Path object if it's a string\n",
    "        folder_path = pathlib.Path(folder_path)\n",
    "\n",
    "        # Check if the provided path is a directory\n",
    "        if folder_path.is_dir():\n",
    "            for entry in folder_path.iterdir():\n",
    "                if entry.is_file() and any(\n",
    "                    entry.suffix.lower() == ext.lower() for ext in file_extensions\n",
    "                ):\n",
    "                    matching_files.append(str(entry))\n",
    "                elif recursive and entry.is_dir():\n",
    "                    # Recursively search subdirectories\n",
    "                    matching_files.extend(\n",
    "                        list_files_by_extension(entry, file_extensions, recursive)\n",
    "                    )\n",
    "        else:\n",
    "            print(f\"The provided path '{folder_path}' is not a directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return matching_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6104aeb-e4cf-45a0-99dd-2d6e248daee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files(input_files, filter_words, exclude_words=None, include_all=True):\n",
    "    \"\"\"\n",
    "    Filters a list of files based on include and exclude words.\n",
    "    Parameters:\n",
    "        input_files (list): List of file paths to be filtered.\n",
    "        filter_words (list): Words that must be present in the filenames for inclusion.\n",
    "        exclude_words (list, optional): Words that must not be present in the filenames for exclusion. Defaults to None.\n",
    "        include_all (bool, optional): If True, all filter words must be present in the filename. If False, at least one of the filter words must be present. Defaults to False.\n",
    "    Returns:\n",
    "        list: Filtered list of files.\n",
    "    \"\"\"\n",
    "    # Ensure all words are lowercase for case-insensitive comparison\n",
    "    filter_words = [word.lower() for word in filter_words]\n",
    "    exclude_words = [word.lower() for word in (exclude_words or [])]\n",
    "\n",
    "    if include_all:\n",
    "        filtered_files = [\n",
    "            file\n",
    "            for file in input_files\n",
    "            if all(word in pathlib.Path(file).name.lower() for word in filter_words)\n",
    "            and not any(\n",
    "                exclude_word in pathlib.Path(file).name.lower()\n",
    "                for exclude_word in exclude_words\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        filtered_files = [\n",
    "            file\n",
    "            for file in input_files\n",
    "            if any(word in pathlib.Path(file).name.lower() for word in filter_words)\n",
    "            and not any(\n",
    "                exclude_word in pathlib.Path(file).name.lower()\n",
    "                for exclude_word in exclude_words\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return filtered_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7e6b7-a73b-4b6d-a699-576996f7c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_ipynb_checkpoints(input_files):\n",
    "    \"\"\"\n",
    "    Filters out files whose paths contain '.ipynb_checkpoints'.\n",
    "    Parameters:\n",
    "        input_files (list): List of file paths to be filtered.\n",
    "    Returns:\n",
    "        list: Filtered list of files.\n",
    "    \"\"\"\n",
    "    filtered_files = [\n",
    "        file\n",
    "        for file in input_files\n",
    "        if \".ipynb_checkpoints\" not in pathlib.Path(file).parts\n",
    "    ]\n",
    "    return filtered_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadd242-9513-4811-a318-39d3ed988546",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = project_folder\n",
    "folders = list_folders(directory_path)\n",
    "available_models = filter_folders(folders, model_to_allocate, [\"data\", \"data_raw\"])\n",
    "available_models = filter_out_ipynb_checkpoints(available_models)\n",
    "print(\"Models_available:\", available_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c67c32-6e2f-4a17-89ef-d0182fa46122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = available_models[0]\n",
    "tif_files = list_files_by_extension(model_folder, [\".tif\"], True)\n",
    "available_prediction_files = filter_files(tif_files, periods, None, False)\n",
    "available_prediction_file = available_prediction_files[0]\n",
    "available_prediction_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12c35e-41d9-498b-a6f0-4964c7bc8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = available_models[0]\n",
    "csv_files = list_files_by_extension(model_folder, [\".csv\"], True)\n",
    "model_files = filter_files(csv_files, periods + [\"defrate\"], None, True)\n",
    "available_defrate_files = filter_out_ipynb_checkpoints(model_files)\n",
    "available_defrate_file = available_defrate_files[0]\n",
    "available_defrate_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba919a-3341-442e-924d-ee870327731d",
   "metadata": {},
   "source": [
    "## Select forest cover change file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d17f5-7b11-481e-8cde-a49d0b51d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all raster files in the processed data folder\n",
    "input_raster_files = list_files_by_extension(processed_data_folder, [\".tiff\", \".tif\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9d584-29bd-436a-a7f1-aa3a9a0e030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_change_file = filter_files(\n",
    "    input_raster_files,\n",
    "    [\"forest\", \"loss\", forest_source] + [str(num) for num in years],\n",
    "    [\"distance\", \"edge\"],\n",
    ")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c886152-5c18-4daf-884b-45434df2b29a",
   "metadata": {},
   "source": [
    "## Allocate deforestation to project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c8ac3-ce8a-4a15-ad1d-4705d35f790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "from fiona.transform import transform_geom\n",
    "\n",
    "\n",
    "def reproject_shapefile(input_shapefile, output_shapefile, target_epsg):\n",
    "    # Open the input shapefile\n",
    "    with fiona.open(input_shapefile, \"r\") as src:\n",
    "        # Get the source CRS\n",
    "        src_crs = src.crs\n",
    "        # Define the target CRS\n",
    "        target_crs = f\"EPSG:{target_epsg}\"\n",
    "\n",
    "        # Check if the source and target CRS are different\n",
    "        if src_crs != target_crs:\n",
    "            # Define the transformation function\n",
    "            project = lambda geom: transform_geom(src_crs, target_crs, geom)\n",
    "            # Open the output shapefile for writing\n",
    "            with fiona.open(\n",
    "                output_shapefile,\n",
    "                \"w\",\n",
    "                crs=\"EPSG:32620\",\n",
    "                driver=\"ESRI Shapefile\",\n",
    "                schema=src.schema,\n",
    "            ) as dst:\n",
    "                # Iterate over features in the source shapefile\n",
    "                for feature in src:\n",
    "                    # Transform the geometry and create a new feature\n",
    "                    new_feature = {**feature, \"geometry\": project(feature[\"geometry\"])}\n",
    "                    # Write the transformed feature to the output shapefile\n",
    "                    dst.write(new_feature)\n",
    "            return output_shapefile  # Return the output shapefile if reprojection was performed\n",
    "        else:\n",
    "            print(\"Source and target CRS are the same. No reprojection needed.\")\n",
    "            return input_shapefile  # Return the input shapefile if no reprojection was needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d82a1-35cf-4a08-91a6-ce62584b4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_borders = allocation_folder / \"project_mtq.shp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e0f46-97af-4df6-b0f1-3f1d1729a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg_code = 32620\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4313b-72af-41cd-aee2-a309bed498db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_name = os.path.splitext(os.path.basename(project_borders))[0]\n",
    "# reprojected_filename = os.path.join(allocation_folder, f\"{base_name}_reprojected.shp\")\n",
    "# project_borders_repro = reproject_shapefile(project_borders,reprojected_filename,epsg_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261648c-0857-41db-a577-e3b479c389a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_borders_repro = allocation_folder / \"project_mtq_repro.shp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07eaeea-5bf2-4cc5-aa28-6df0f94be900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "import forestatrisk\n",
    "\n",
    "# Local application imports\n",
    "from forestatrisk.misc import progress_bar, makeblock\n",
    "\n",
    "opj = os.path.join\n",
    "opd = os.path.dirname\n",
    "\n",
    "\n",
    "def allocate_deforestation(\n",
    "    riskmap_juris_file,\n",
    "    defor_rate_tab,\n",
    "    defor_juris_ha,\n",
    "    years_forecast,\n",
    "    project_borders,\n",
    "    output_file=\"defor_project.csv\",\n",
    "    defor_density_map=False,\n",
    "    blk_rows=128,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Allocating deforestation.\n",
    "\n",
    "    :param riskmap_juris_file: Raster file with classes of deforestation\n",
    "      risk at the jurisdictional level.\n",
    "\n",
    "    :param defor_rate_tab: CSV file including the table with\n",
    "      deforestation rates for each deforestation class.\n",
    "\n",
    "    :param defor_juris_ha: Expected deforestation at the\n",
    "      jurisdictional level (in hectares).\n",
    "\n",
    "    :param years_forecast: Length of the forecasting period (in years).\n",
    "\n",
    "    :param project_borders: Vector file for project borders.\n",
    "\n",
    "    :param output_file: Output file with deforestation\n",
    "      allocated to the project.\n",
    "\n",
    "    :param defor_density_map: Compute the deforestation density map\n",
    "      for the jurisdiction. Deforestation density is provided in\n",
    "      ha/pixel/year (hectares of deforestation per pixel per year).\n",
    "      Deforestation densities are floating-point numbers. For large\n",
    "      jurisdictions (e.g. country scale) and high resolutions (e.g. 30\n",
    "      m), this will produce a large raster file which will occupy\n",
    "      a large amount of space on disk (e.g. several gigabytes).\n",
    "\n",
    "    :param blk_rows: If > 0, number of rows for block (else 256x256).\n",
    "\n",
    "    :param verbose: If True, print messages.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Callback\n",
    "    cback = gdal.TermProgress_nocb if verbose else 0\n",
    "\n",
    "    # Creation options\n",
    "    copts = [\"COMPRESS=DEFLATE\", \"BIGTIFF=YES\"]\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Crop riskmap to project boundaries\n",
    "    # ---------------------------------------\n",
    "\n",
    "    out_dir = opd(output_file)\n",
    "    ofile = opj(out_dir, \"project_riskmap.tif\")\n",
    "    gdal.Warp(\n",
    "        ofile,\n",
    "        riskmap_juris_file,\n",
    "        cropToCutline=True,\n",
    "        warpOptions=[\"CUTLINE_ALL_TOUCHED=TRUE\"],\n",
    "        cutlineDSName=project_borders,\n",
    "        creationOptions=copts,\n",
    "        callback=cback,\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Compute number of pixels for each class\n",
    "    # ---------------------------------------\n",
    "\n",
    "    nvalues = 65535\n",
    "    with gdal.Open(ofile) as ds:\n",
    "        band = ds.GetRasterBand(1)\n",
    "        counts = band.GetHistogram(0.5, 65535.5, nvalues, 0, 0)\n",
    "    data = {\"cat\": [i + 1 for i in range(65535)], \"counts\": counts}\n",
    "    df_count = pd.DataFrame(data)\n",
    "\n",
    "    # Upload deforestation rates\n",
    "    df_rate = pd.read_csv(defor_rate_tab)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Compute deforestation density\n",
    "    # -----------------------------\n",
    "\n",
    "    # Pixel area\n",
    "    pixel_area = df_rate.loc[0, \"pixel_area\"]\n",
    "\n",
    "    # Correction factor, either ndefor / sum_i p_i\n",
    "    # or theta * nfor / sum_i p_i\n",
    "    sum_pi = (df_rate[\"nfor\"] * df_rate[\"rate_mod\"]).sum()\n",
    "    correction_factor = defor_juris_ha / (pixel_area * sum_pi)\n",
    "\n",
    "    # Absolute deforestation rate\n",
    "    df_rate[\"rate_abs\"] = df_rate[\"rate_mod\"] * correction_factor\n",
    "\n",
    "    # Deforestation density (ha/pixel/yr)\n",
    "    df_rate[\"defor_dens\"] = df_rate[\"rate_abs\"] * pixel_area / years_forecast\n",
    "\n",
    "    # Save the df_rate table\n",
    "    ofile = opj(out_dir, \"defrate_cat_forecast.csv\")\n",
    "    df_rate.to_csv(ofile)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Join tables\n",
    "    # -----------------------------\n",
    "\n",
    "    df_project = df_count.merge(right=df_rate, on=\"cat\", how=\"left\")\n",
    "\n",
    "    # Annual deforestation (ha) for project\n",
    "    defor_project = (df_project[\"counts\"] * df_project[\"defor_dens\"]).sum()\n",
    "\n",
    "    # Save results\n",
    "    data = {\n",
    "        \"period\": [\"annual\", \"entire\"],\n",
    "        \"length (yr)\": [1, years_forecast],\n",
    "        \"deforestation (ha)\": [\n",
    "            round(defor_project, 1),\n",
    "            round(defor_project * years_forecast, 1),\n",
    "        ],\n",
    "    }\n",
    "    res = pd.DataFrame(data)\n",
    "    res.to_csv(output_file, header=True, index=False)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Get deforestation density map\n",
    "    # -----------------------------\n",
    "\n",
    "    if defor_density_map:\n",
    "        riskmap_r = gdal.Open(riskmap_juris_file)\n",
    "        riskmap_b = riskmap_r.GetRasterBand(1)\n",
    "        gt = riskmap_r.GetGeoTransform()\n",
    "        proj = riskmap_r.GetProjection()\n",
    "        ncol = riskmap_r.RasterXSize\n",
    "        nrow = riskmap_r.RasterYSize\n",
    "\n",
    "        output_file = opj(out_dir, \"deforestation_density_map.tif\")\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        if os.path.isfile(output_file):\n",
    "            os.remove(output_file)\n",
    "        ddm_r = driver.Create(\n",
    "            output_file,\n",
    "            ncol,\n",
    "            nrow,\n",
    "            1,\n",
    "            gdal.GDT_Float64,\n",
    "            [\"COMPRESS=DEFLATE\", \"BIGTIFF=YES\"],\n",
    "        )\n",
    "        ddm_r.SetGeoTransform(gt)\n",
    "        ddm_r.SetProjection(proj)\n",
    "        ddm_b = ddm_r.GetRasterBand(1)\n",
    "        ddm_b.SetNoDataValue(-9999.0)\n",
    "\n",
    "        # Make blocks\n",
    "        blockinfo = makeblock(riskmap_juris_file, blk_rows=blk_rows)\n",
    "        nblock = blockinfo[0]\n",
    "        nblock_x = blockinfo[1]\n",
    "        x = blockinfo[3]\n",
    "        y = blockinfo[4]\n",
    "        nx = blockinfo[5]\n",
    "        ny = blockinfo[6]\n",
    "        if verbose:\n",
    "            print(f\"Divide region in {nblock} blocks\")\n",
    "\n",
    "        # Write raster of dd\n",
    "        if verbose:\n",
    "            print(\"Write deforestation density raster\")\n",
    "        # Loop on blocks of data\n",
    "        for b in range(nblock):\n",
    "            # Progress bar\n",
    "            progress_bar(nblock, b + 1)\n",
    "            # Position in 1D-arrays\n",
    "            px = b % nblock_x\n",
    "            py = b // nblock_x\n",
    "            # Data for one block\n",
    "            risk_data = riskmap_b.ReadAsArray(x[px], y[py], nx[px], ny[py])\n",
    "            risk_data = risk_data.flatten(order=\"C\")\n",
    "            # Get defor density from risk class\n",
    "            defor_dens = np.zeros(len(risk_data))\n",
    "            defor_dens[risk_data == 0] = -9999.0\n",
    "\n",
    "            non_zero_risk_classes = risk_data[risk_data != 0]\n",
    "\n",
    "            # Make sure df_rate has 'cat' as index (do this once after reading)\n",
    "            df_rate_indexed = df_rate.set_index(\"cat\")\n",
    "\n",
    "            defor_dens_values = pd.Series(non_zero_risk_classes).map(\n",
    "                df_rate_indexed[\"defor_dens\"]\n",
    "            )\n",
    "\n",
    "            if defor_dens_values.isna().any():\n",
    "                missing = non_zero_risk_classes[defor_dens_values.isna()]\n",
    "                print(\n",
    "                    f\"Warning: Missing deforestation density for risk classes: {missing.unique()}\"\n",
    "                )\n",
    "\n",
    "            # Fill missing with 0 (or -9999, or raise an error)\n",
    "            defor_dens_values = defor_dens_values.fillna(0.0)\n",
    "\n",
    "            defor_dens[risk_data != 0] = defor_dens_values.values\n",
    "            defor_dens = defor_dens.reshape((ny[py], nx[px]), order=\"C\")\n",
    "            # Write deforestation densities\n",
    "            ddm_b.WriteArray(defor_dens, x[px], y[py])\n",
    "\n",
    "        # Compute statistics\n",
    "        if verbose:\n",
    "            print(\"Compute statistics\")\n",
    "        ddm_b.FlushCache()  # Write cache data to disk\n",
    "        ddm_b.ComputeStatistics(False)\n",
    "\n",
    "        # Dereference gdal datasets\n",
    "        riskmap_b = None\n",
    "        ddm_b = None\n",
    "        del riskmap_r, ddm_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a11e2-5dd4-4444-8e5c-3306857ca6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import forestatrisk as far\n",
    "\n",
    "\n",
    "def allocate_deforestation2(\n",
    "    available_prediction_file,\n",
    "    available_defrate_file,\n",
    "    jurisdiction_expected_total_deforestation_ha,\n",
    "    years_to_forecast,\n",
    "    project_borders,\n",
    "):\n",
    "    allocate_deforestation(\n",
    "        riskmap_juris_file=available_prediction_file,\n",
    "        defor_rate_tab=available_defrate_file,\n",
    "        defor_juris_ha=jurisdiction_expected_total_deforestation_ha,\n",
    "        years_forecast=years_to_forecast,\n",
    "        project_borders=project_borders,\n",
    "        output_file=allocation_folder / \"defor_project.csv\",\n",
    "        defor_density_map=True,\n",
    "        blk_rows=256,\n",
    "        verbose=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea416463-3912-4c86-b864-684def3d708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "allocate_deforestation2(\n",
    "    available_prediction_file,\n",
    "    available_defrate_file,\n",
    "    jurisdiction_expected_total_deforestation_ha,\n",
    "    years_to_forecast,\n",
    "    project_borders_repro,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1838135-023f-4d2f-b189-5b69a51ecca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
