{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93956c07-d9f1-4e3a-ae1b-148aec301334",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896caed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "# import pandas as pd\n",
    "# print(pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cuml\n",
    "# cuml.accel.install()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c599305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizations\n",
    "# GDAL optimizations\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "cpu_count: int = mp.cpu_count()\n",
    "num_cores: int = cpu_count - 2\n",
    "os.environ[\"GDAL_NUM_THREADS\"] = f\"{num_cores}\"\n",
    "os.environ[\"GDAL_CACHEMAX\"] = \"1024\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2a25b-11a9-436c-9eff-17ad4003f00c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13a12a-7502-4c76-b490-9880fbf99c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import riskmapjnr as rmj\n",
    "from tabulate import tabulate\n",
    "from patsy import dmatrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15850cf5-6362-4e08-aef7-f9372ff90576",
   "metadata": {},
   "source": [
    "## Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106736a8-74db-4767-b3c2-f7c398b97aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_source = \"gfc\"  ##gfc, tmf\n",
    "tree_cover_threshold = 10\n",
    "years = [2015, 2020, 2024]\n",
    "string_years = [str(num) for num in years]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01fff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csize = 10\n",
    "prior_vrho = -1\n",
    "mcmc = 10000\n",
    "thin = 1\n",
    "beta_start = -99\n",
    "random_seed = 1\n",
    "csize_interpolate = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928f394",
   "metadata": {},
   "source": [
    "## Connect folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder: Path = Path.cwd().parent\n",
    "downloads_folder: Path = root_folder / \"data\"\n",
    "downloads_folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = downloads_folder / project_name\n",
    "project_folder.mkdir(parents=True, exist_ok=True)\n",
    "processed_data_folder = project_folder / \"data\"\n",
    "processed_data_folder.mkdir(parents=True, exist_ok=True)\n",
    "sampling_folder = project_folder / \"far_samples\"\n",
    "sampling_folder.mkdir(parents=True, exist_ok=True)\n",
    "icar_model = project_folder / \"far_icar\"\n",
    "icar_model.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a8771",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_by_extension(folder_path, file_extensions, recursive=False):\n",
    "    \"\"\"\n",
    "    List all files with specified extensions in the given folder.\n",
    "    Parameters:\n",
    "    folder_path (str or Path): The path to the folder where you want to search for files.\n",
    "    file_extensions (list of str): A list of file extensions to search for (e.g., ['.shp', '.tif']).\n",
    "    recursive (bool): Whether to recursively search through subdirectories or not.\n",
    "    Returns:\n",
    "    list: A list of file paths with the specified extensions.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "    try:\n",
    "        # Convert folder_path to Path object if it's a string\n",
    "        folder_path = Path(folder_path)\n",
    "\n",
    "        # Check if the provided path is a directory\n",
    "        if folder_path.is_dir():\n",
    "            for entry in folder_path.iterdir():\n",
    "                if entry.is_file() and any(\n",
    "                    entry.suffix.lower() == ext.lower() for ext in file_extensions\n",
    "                ):\n",
    "                    matching_files.append(str(entry))\n",
    "                elif recursive and entry.is_dir():\n",
    "                    # Recursively search subdirectories\n",
    "                    matching_files.extend(\n",
    "                        list_files_by_extension(entry, file_extensions, recursive)\n",
    "                    )\n",
    "        else:\n",
    "            print(f\"The provided path '{folder_path}' is not a directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return matching_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33962291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_files(input_files, filter_words, exclude_words=None, include_all=True):\n",
    "    \"\"\"\n",
    "    Filters a list of files based on include and exclude words.\n",
    "    Parameters:\n",
    "        input_files (list): List of file paths to be filtered.\n",
    "        filter_words (list): Words that must be present in the filenames for inclusion.\n",
    "        exclude_words (list, optional): Words that must not be present in the filenames for exclusion. Defaults to None.\n",
    "        include_all (bool, optional): If True, all filter words must be present in the filename. If False, at least one of the filter words must be present. Defaults to False.\n",
    "    Returns:\n",
    "        list: Filtered list of files.\n",
    "    \"\"\"\n",
    "    # Ensure all words are lowercase for case-insensitive comparison\n",
    "    filter_words = [word.lower() for word in filter_words]\n",
    "    exclude_words = [word.lower() for word in (exclude_words or [])]\n",
    "\n",
    "    if include_all:\n",
    "        filtered_files = [\n",
    "            file\n",
    "            for file in input_files\n",
    "            if all(word in Path(file).name.lower() for word in filter_words)\n",
    "            and not any(\n",
    "                exclude_word in Path(file).name.lower()\n",
    "                for exclude_word in exclude_words\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        filtered_files = [\n",
    "            file\n",
    "            for file in input_files\n",
    "            if any(word in Path(file).name.lower() for word in filter_words)\n",
    "            and not any(\n",
    "                exclude_word in Path(file).name.lower()\n",
    "                for exclude_word in exclude_words\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return filtered_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_ipynb_checkpoints(input_files):\n",
    "    \"\"\"\n",
    "    Filters out files whose paths contain '.ipynb_checkpoints'.\n",
    "    Parameters:\n",
    "        input_files (list): List of file paths to be filtered.\n",
    "    Returns:\n",
    "        list: Filtered list of files.\n",
    "    \"\"\"\n",
    "    filtered_files = [\n",
    "        file for file in input_files if \".ipynb_checkpoints\" not in Path(file).parts\n",
    "    ]\n",
    "    return filtered_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba919a-3341-442e-924d-ee870327731d",
   "metadata": {},
   "source": [
    "## Select forest cover change file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d17f5-7b11-481e-8cde-a49d0b51d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all raster files in the processed data folder\n",
    "input_raster_files = filter_out_ipynb_checkpoints(\n",
    "    list_files_by_extension(processed_data_folder, [\".tiff\", \".tif\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9d584-29bd-436a-a7f1-aa3a9a0e030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_change_file = filter_files(\n",
    "    input_raster_files,\n",
    "    [\"forest\", \"loss\", forest_source] + [str(num) for num in years],\n",
    "    [\"distance\", \"edge\"],\n",
    ")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27afa2dd-3ec8-4a09-a13c-dccfb72fad40",
   "metadata": {},
   "source": [
    "## Periods dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2eff92-7905-4f52-b1f2-cc97ef54bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_dict = {\n",
    "    \"period\": \"calibration\",\n",
    "    \"train_period\": \"calibration\",\n",
    "    \"initial_year\": years[0],\n",
    "    \"final_year\": years[1],\n",
    "    \"defor_value\": 1,\n",
    "    \"time_interval\": years[1] - years[0],\n",
    "}\n",
    "validation_dict = {\n",
    "    \"period\": \"validation\",\n",
    "    \"train_period\": \"calibration\",\n",
    "    \"initial_year\": years[1],\n",
    "    \"final_year\": years[2],\n",
    "    \"defor_value\": 1,\n",
    "    \"time_interval\": years[2] - years[1],\n",
    "}\n",
    "historical_dict = {\n",
    "    \"period\": \"historical\",\n",
    "    \"train_period\": \"historical\",\n",
    "    \"initial_year\": years[0],\n",
    "    \"final_year\": years[2],\n",
    "    \"defor_value\": [1, 2],\n",
    "    \"time_interval\": years[2] - years[0],\n",
    "}\n",
    "forecast_dict = {\n",
    "    \"period\": \"forecast\",\n",
    "    \"train_period\": \"historical\",\n",
    "    \"initial_year\": years[0],\n",
    "    \"final_year\": years[2],\n",
    "    \"defor_value\": [1, 2],\n",
    "    \"time_interval\": years[2] - years[0],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e29cd3-018c-4bc0-b851-5a4acc7cab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el diccionario principal\n",
    "period_dictionaries = {\n",
    "    calibration_dict[\"period\"]: calibration_dict,\n",
    "    validation_dict[\"period\"]: validation_dict,\n",
    "    historical_dict[\"period\"]: historical_dict,\n",
    "    forecast_dict[\"period\"]: forecast_dict,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af66ccc-8e4d-44be-8e21-884eab03a3b1",
   "metadata": {},
   "source": [
    "## Select input files based on period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739a47b-e2b1-489d-b34c-e0afa56a426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fcc_files(input_raster_files, period_dict, period):\n",
    "    # Define the period-dependent variables and their associated files\n",
    "    period_dictionary = period_dict[period]\n",
    "    initial_year = str(period_dictionary[\"initial_year\"])\n",
    "    final_year = str(period_dictionary[\"final_year\"])\n",
    "    exclude_year = \", \".join(\n",
    "        map(\n",
    "            str,\n",
    "            set(years)\n",
    "            - {period_dictionary[\"initial_year\"], period_dictionary[\"final_year\"]},\n",
    "        )\n",
    "    )\n",
    "    forest_loss_files = filter_files(\n",
    "        input_raster_files,\n",
    "        [forest_source, initial_year, final_year, \"forest\", \"loss\"],\n",
    "        [exclude_year, \"edge\"],\n",
    "        True,\n",
    "    )\n",
    "\n",
    "    # Create a dictionary with variable types as keys and file paths as values\n",
    "    variable_file_mapping = {\n",
    "        \"period\": period_dictionary[\"period\"],\n",
    "        \"fcc\": forest_loss_files[0],\n",
    "    }\n",
    "    return variable_file_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6820e-ea17-4568-a08b-23c45fdbdebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_for_period(period, sampling_folder):\n",
    "    period_name = period_dictionaries[period][\"train_period\"]\n",
    "    samples = sampling_folder / period_name / \"sample.txt\"\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ffac0-92f7-4bfd-9ee9-cafb222b21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csize_for_period(period, sampling_folder):\n",
    "    period_name = period_dictionaries[period][\"train_period\"]\n",
    "    samples = sampling_folder / period_name / \"csize_icar.txt\"\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9deb9e9-60b6-4ec1-ab5c-3d4ba90a76d7",
   "metadata": {},
   "source": [
    "## Train icar based on period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f987e1-f23c-445e-9ab9-8e173c945e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_raw_variables(formula: str) -> set:\n",
    "    \"\"\"\n",
    "    Extract raw variable names from a Patsy-style formula,\n",
    "    safely handling I(), scale(), C(), and other transformations.\n",
    "\n",
    "    Example:\n",
    "        \"I(1 - fcc) + trial ~ scale(altitude) + C(pa)\"\n",
    "        → returns {'fcc', 'trial', 'altitude', 'pa'}\n",
    "    \"\"\"\n",
    "    raw_vars = set()\n",
    "\n",
    "    # Pattern to match: any Patsy function with content inside parentheses\n",
    "    # We capture the inner part, then extract variables from it\n",
    "    pattern = r\"[a-zA-Z_][a-zA-Z0-9_]*\\(([^)]+)\\)\"\n",
    "\n",
    "    # Find all expressions like I(...), scale(...), C(...)\n",
    "    matches = re.findall(pattern, formula)\n",
    "\n",
    "    for expr in matches:\n",
    "        # Clean the expression: remove spaces, split by operators\n",
    "        # We want to extract only variable names (no constants or math)\n",
    "        tokens = re.split(r\"[+\\-*/\\(\\)\\s]\", expr)  # Split on common symbols\n",
    "        tokens = [t.strip() for t in tokens if t.strip()]\n",
    "\n",
    "        # Keep only valid identifiers that are not numbers/strings\n",
    "        for token in tokens:\n",
    "            # Skip numeric literals (e.g., '1', '2.3')\n",
    "            if re.match(r\"^\\d+(\\.\\d+)?$\", token):\n",
    "                continue\n",
    "            # Skip keywords like 'I', 'scale'\n",
    "            if token.lower() in {\"i\", \"scale\", \"c\", \"poly\", \"bs\", \"cr\"}:\n",
    "                continue\n",
    "            raw_vars.add(token)\n",
    "\n",
    "    # Now extract standalone variables (not inside functions)\n",
    "    standalone = re.findall(r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\b\", formula)\n",
    "\n",
    "    for var in standalone:\n",
    "        if var.lower() not in {\"i\", \"scale\", \"c\"}:  # Skip Patsy keywords\n",
    "            raw_vars.add(var)\n",
    "\n",
    "    # Remove invalid tokens (e.g., '1-fcc' is not a column name)\n",
    "    raw_vars = {v for v in raw_vars if re.match(r\"^[a-zA-Z_][a-zA-Z0-9_]*$\", v)}\n",
    "\n",
    "    return raw_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b982b-155a-43c7-aca8-2e9ea4bb7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_for_training = [\n",
    "    \"scale(altitude)\",\n",
    "    \"scale(dist_edge)\",\n",
    "    \"scale(dist_river)\",\n",
    "    \"scale(dist_road)\",\n",
    "    \"scale(dist_town)\",\n",
    "    \"scale(slope)\",\n",
    "    \"C(pa)\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665543f-a013-40ed-ba38-f17bdd3ff468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import forestatrisk\n",
    "\n",
    "\n",
    "def train_icar_from_formula(\n",
    "    formula: str,\n",
    "    dataset_file: str,\n",
    "    raster_path: str,\n",
    "    csize: float,\n",
    "    prior_vrho: int = -1,\n",
    "    burnin: int = 2000,\n",
    "    mcmc: int = 1000,\n",
    "    thin: int = 1,\n",
    "    beta_start=-99,\n",
    "    random_state: int = 42,\n",
    "    model_file: str = \"icar_model.pickle\",\n",
    "    summary_file: str = \"summary_icar.txt\",\n",
    "    mcmc_file: str = \"mcmc.pdf\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a Bayesian iCAR model using the forestatrisk library.\n",
    "\n",
    "    Parameters:\n",
    "        formula (str): Patsy-style formula, e.g., 'I(1-fcc) + trial ~ scale(var1) + C(var2) + cell'\n",
    "        dataset (pd.DataFrame): Input data with columns matching formula\n",
    "        raster_path (str): Path to a GeoTIFF file for spatial neighborhood (e.g., \"fcc.tif\")\n",
    "        csize (float): Cell size in map units (used for neighbor distance)\n",
    "        prior_vrho (float): Prior variance for rho (spatial autocorrelation parameter), default=1.0\n",
    "        burnin (int): Number of burn-in MCMC samples\n",
    "        mcmc (int): Total number of MCMC iterations\n",
    "        thin (int): Thinning interval (keep every 'thin' sample)\n",
    "        beta_start (array-like, optional): Initial values for coefficients; if None, uses default\n",
    "        random_state (int): Seed for reproducibility\n",
    "        output_dir (str): Directory to save outputs (summary.txt, mcmc.pdf, model.pkl)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with model results and paths to saved files\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the dataset from the text file\n",
    "    print(f\"📊 Loading data from {dataset_file}...\")\n",
    "    try:\n",
    "        dataset = pd.read_csv(dataset_file)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to read dataset file: {e}\")\n",
    "\n",
    "    if dataset.empty:\n",
    "        raise ValueError(\"Dataset is empty after loading.\")\n",
    "\n",
    "    # Apply required preprocessing\n",
    "    print(\"🧹 Preprocessing data: dropping missing values and adding 'trial' column...\")\n",
    "    dataset = dataset.dropna(axis=0)  # Drop any rows with NA\n",
    "    # dataset = dataset.fillna(0)  # Fill na values from distance files\n",
    "    dataset[\"trial\"] = 1  # Add trial column as 1\n",
    "\n",
    "    # Extract raw variable names used in the formula (ignoring I(), scale(), C())\n",
    "    raw_variables = extract_raw_variables(formula)\n",
    "\n",
    "    # Also ensure that `trial` and `cell` are present — these are often used as offsets or weights\n",
    "    required_vars = raw_variables | {\"trial\", \"cell\"}\n",
    "\n",
    "    # Check which required variables are missing from dataset\n",
    "    missing_vars = [var for var in required_vars if var not in dataset.columns]\n",
    "\n",
    "    if missing_vars:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_vars}\")\n",
    "\n",
    "    print(len(dataset))\n",
    "\n",
    "    # Now filter the dataset: keep only relevant columns\n",
    "    try:\n",
    "        dataset = dataset[list(required_vars)]\n",
    "    except KeyError as e:\n",
    "        raise ValueError(f\"Failed to select columns from dataset: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"💾 Filtered dataset to {len(dataset.columns)} variables: {list(dataset.columns)}\"\n",
    "    )\n",
    "\n",
    "    # Step 1: Parse formula using patsy\n",
    "    y, x = dmatrices(formula, data=dataset, NA_action=\"drop\")\n",
    "    # Ensure consistent preprocessing\n",
    "    # Debug: Confirm alignment\n",
    "    if len(y) != len(x):\n",
    "        raise ValueError(\n",
    "            f\"Inconsistent sample sizes after dmatrices: Y={len(y)}, X={len(x)}\"\n",
    "        )\n",
    "\n",
    "    Y = y[:, 0]\n",
    "    X = x\n",
    "\n",
    "    # Step 2: Compute spatial neighborhood from raster\n",
    "    # Use forestatrisk's cellneigh to get adjacency structure\n",
    "    nneigh, adj = forestatrisk.cellneigh(\n",
    "        raster=raster_path,\n",
    "        csize=csize,\n",
    "        rank=1,  # Adjacency based on 4-neighbors (can change if needed)\n",
    "    )\n",
    "    print(f\"✅ Spatial neighborhood computed\")\n",
    "\n",
    "    # Step 3: Round mcmc to thounds and thin according to it\n",
    "    if mcmc >= 1000:\n",
    "        mcmc = int(mcmc // 1000) * 1000\n",
    "        thin = int(mcmc / 1000)\n",
    "    else:\n",
    "        mcmc = mcmc\n",
    "        thin = 1\n",
    "\n",
    "    # Step 3: Train iCAR model via forestatrisk\n",
    "    mod_icar = forestatrisk.model_binomial_iCAR(\n",
    "        suitability_formula=formula,\n",
    "        data=dataset,\n",
    "        n_neighbors=nneigh,\n",
    "        neighbors=adj,\n",
    "        priorVrho=prior_vrho,\n",
    "        burnin=mcmc,\n",
    "        mcmc=mcmc,\n",
    "        thin=thin,\n",
    "        beta_start=beta_start,\n",
    "    )\n",
    "\n",
    "    # print(f\"✅ iCAR model trained. Deviance={deviance}, rho={rho}\")\n",
    "    print(f\"✅ iCAR model trained.\")\n",
    "\n",
    "    # Step 4: Extract results and Save model metadata (pickle)\n",
    "    model_data = {\n",
    "        \"formula\": mod_icar.suitability_formula,\n",
    "        \"rho\": mod_icar.rho,\n",
    "        \"betas\": mod_icar.betas,\n",
    "        \"Vrho\": mod_icar.Vrho,\n",
    "        \"deviance\": mod_icar.deviance,\n",
    "    }\n",
    "\n",
    "    with open(model_file, \"wb\") as file:\n",
    "        pickle.dump(model_data, file)\n",
    "    print(f\"💾 iCAR model metadata saved to: {model_file}\")\n",
    "\n",
    "    # Step 5: Save summary\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(mod_icar))\n",
    "    print(f\"📄 Summary saved to: {summary_file}\")\n",
    "\n",
    "    # Step 6: Plot MCMC diagnostics\n",
    "    # figs = mod_icar.plot(\n",
    "    #     output_file= str(mcmc_file),\n",
    "    #     plots_per_page=3,\n",
    "    #     figsize=(10, 6),\n",
    "    #     dpi=80\n",
    "    # )\n",
    "    # for fig in figs:\n",
    "    #     plt.close(fig)\n",
    "    # print(f\"📊 MCMC diagnostics saved to: {str(mcmc_file)}\")\n",
    "\n",
    "    # Step 7: Return results\n",
    "    return {\n",
    "        \"model\": mod_icar,\n",
    "        \"summary_file\": str(summary_file),\n",
    "        \"mcmc_file\": str(mcmc_file),\n",
    "        \"model_data_file\": str(model_file),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16648d5d-9f26-4fbd-9d2d-72e905a8f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_icar_period(\n",
    "    variables,\n",
    "    input_raster_files,\n",
    "    period_dictionary,\n",
    "    period,\n",
    "    sampling_folder,\n",
    "    model_folder,\n",
    "    csize,\n",
    "    prior_vrho,\n",
    "    mcmc,\n",
    "    burnin,\n",
    "    thin,\n",
    "    beta_start,\n",
    "    random_seed,\n",
    "    csize_interpolate,\n",
    "):\n",
    "    ##Get formula\n",
    "    right_part = \" + \".join(variables) + \" + cell\"\n",
    "    left_part = \"I(1-fcc) + trial ~ \"\n",
    "    # left_part = \"I(fcc) + trial ~ \"\n",
    "    icar_formula = left_part + right_part\n",
    "    # Get samples\n",
    "    samples_path = get_samples_for_period(period, sampling_folder)\n",
    "    # Raster fcc\n",
    "    fcc_raster = get_fcc_files(input_raster_files, period_dictionary, \"calibration\")[\n",
    "        \"fcc\"\n",
    "    ]\n",
    "    # Create period folder\n",
    "    period_output_folder = model_folder / period\n",
    "    if not os.path.exists(period_output_folder):\n",
    "        os.makedirs(period_output_folder)\n",
    "    # Set outputfiles\n",
    "    model_output = period_output_folder / \"icar_model.pickle\"\n",
    "    summary_file = period_output_folder / \"summary_icar.txt\"\n",
    "    mcmc_file = period_output_folder / \"mcmc.pdf\"\n",
    "    rho_file = str(period_output_folder / \"rho.tif\")\n",
    "\n",
    "    # Train ICAR\n",
    "    icar_trined = train_icar_from_formula(\n",
    "        icar_formula,\n",
    "        samples_path,\n",
    "        fcc_raster,\n",
    "        csize,\n",
    "        prior_vrho,\n",
    "        mcmc,\n",
    "        mcmc,\n",
    "        thin,\n",
    "        beta_start,\n",
    "        random_seed,\n",
    "        model_output,\n",
    "        summary_file,\n",
    "        mcmc_file,\n",
    "    )\n",
    "    # Get csize\n",
    "    csize_file = get_csize_for_period(period, sampling_folder)\n",
    "\n",
    "    # Rho interpolation\n",
    "    with open(str(model_output), \"rb\") as file:\n",
    "        mod_icar_pickle = pickle.load(file)\n",
    "    rho = mod_icar_pickle[\"rho\"]\n",
    "    rho_intherpolation = forestatrisk.interpolate_rho(\n",
    "        rho=rho,\n",
    "        input_raster=fcc_raster,\n",
    "        output_file=rho_file,\n",
    "        csize_orig=csize,\n",
    "        csize_new=csize_interpolate,\n",
    "    )\n",
    "    return icar_trined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b696544-036d-423d-9e81-21d214daec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_icar = train_icar_period(\n",
    "    variables_for_training,\n",
    "    input_raster_files,\n",
    "    period_dictionaries,\n",
    "    \"calibration\",\n",
    "    sampling_folder,\n",
    "    icar_model,\n",
    "    csize,\n",
    "    prior_vrho,\n",
    "    mcmc,\n",
    "    mcmc,\n",
    "    thin,\n",
    "    beta_start,\n",
    "    random_seed,\n",
    "    csize_interpolate,\n",
    ")\n",
    "calibration_icar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cbc2b9-24d4-457e-9014-e438aef485d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_icar = train_icar_period(\n",
    "    variables_for_training,\n",
    "    input_raster_files,\n",
    "    period_dictionaries,\n",
    "    \"historical\",\n",
    "    sampling_folder,\n",
    "    icar_model,\n",
    "    csize,\n",
    "    prior_vrho,\n",
    "    mcmc,\n",
    "    mcmc,\n",
    "    thin,\n",
    "    beta_start,\n",
    "    random_seed,\n",
    "    csize_interpolate,\n",
    ")\n",
    "historical_icar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f02c8",
   "metadata": {},
   "source": [
    "## Select input files based on period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period_variable_files(input_raster_files, period_dict, period):\n",
    "    # Define the period-independent variables and their associated files\n",
    "    period_independant_variables = [\"altitude\", \"slope\", \"pa\", \"subj\"]\n",
    "    altitude_files = filter_files(input_raster_files, [\"altitude\"], None, False)\n",
    "    slope_files = filter_files(input_raster_files, [\"slope\"], None, False)\n",
    "    wdpa_files = filter_files(input_raster_files, [\"pa\"], None, False)\n",
    "    subj_files = filter_files(input_raster_files, [\"subj\"], None, False)\n",
    "\n",
    "    # Define the rivers and roads variables and their associated files\n",
    "    rivers_files = filter_files(\n",
    "        input_raster_files, [\"rivers\", \"reprojected\", \"distance\"], None, True\n",
    "    )\n",
    "    road_files = filter_files(\n",
    "        input_raster_files, [\"roads\", \"reprojected\", \"distance\"], None, True\n",
    "    )\n",
    "\n",
    "    # Define the period-dependent variables and their associated files\n",
    "    period_dictionary = period_dict[period]\n",
    "    initial_year = str(period_dictionary[\"initial_year\"])\n",
    "    final_year = str(period_dictionary[\"final_year\"])\n",
    "    exclude_year = \", \".join(\n",
    "        map(\n",
    "            str,\n",
    "            set(years)\n",
    "            - {period_dictionary[\"initial_year\"], period_dictionary[\"final_year\"]},\n",
    "        )\n",
    "    )\n",
    "    forest_loss_files = filter_files(\n",
    "        input_raster_files,\n",
    "        [forest_source, initial_year, final_year, \"forest\", \"loss\"],\n",
    "        [exclude_year, \"edge\"],\n",
    "        True,\n",
    "    )\n",
    "    # forest_edge_files = filter_files(input_raster_files, [forest_source, initial_year, 'forest','reprojected', 'edge'], None, True)\n",
    "    town_files = filter_files(\n",
    "        input_raster_files,\n",
    "        [initial_year, \"town\", \"reprojected\", \"distance\"],\n",
    "        None,\n",
    "        True,\n",
    "    )\n",
    "    if period in [\"calibration\", \"validation\", \"historical\"]:\n",
    "        forest_files = filter_files(\n",
    "            input_raster_files,\n",
    "            [forest_source, initial_year, \"forest\", \"reprojected\"],\n",
    "            [\"edge\", \"loss\"],\n",
    "            True,\n",
    "        )\n",
    "        forest_edge_files = filter_files(\n",
    "            input_raster_files,\n",
    "            [forest_source, initial_year, \"forest\", \"reprojected\", \"edge\"],\n",
    "            None,\n",
    "            True,\n",
    "        )\n",
    "    elif period == \"forecast\":\n",
    "        forest_files = filter_files(\n",
    "            input_raster_files,\n",
    "            [forest_source, final_year, \"forest\", \"reprojected\"],\n",
    "            [\"edge\"],\n",
    "            True,\n",
    "        )\n",
    "        forest_edge_files = filter_files(\n",
    "            input_raster_files,\n",
    "            [forest_source, final_year, \"forest\", \"reprojected\", \"edge\"],\n",
    "            None,\n",
    "            True,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid period: {period}. Must be 'calibration', 'validation', 'historical', or 'forecast'\"\n",
    "        )\n",
    "    # Create a dictionary with variable types as keys and file paths as values\n",
    "    variable_file_mapping = {\n",
    "        \"period\": period_dictionary[\"period\"],\n",
    "        \"altitude\": altitude_files[0],\n",
    "        \"slope\": slope_files[0],\n",
    "        \"pa\": wdpa_files[0],\n",
    "        \"subj\": subj_files[0],\n",
    "        \"dist_river\": rivers_files[0],\n",
    "        \"dist_road\": road_files[0],\n",
    "        \"dist_town\": town_files[0],\n",
    "        \"fcc\": forest_loss_files[0],\n",
    "        \"dist_edge\": forest_edge_files[0],\n",
    "        \"forest\": forest_files[0],\n",
    "    }\n",
    "    return variable_file_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e249b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(period_dictionaries, period, model_folder):\n",
    "    period_name = period_dictionaries[period][\"train_period\"]\n",
    "    model_period_folder = model_folder / period_name\n",
    "    model = list_files_by_extension(model_period_folder, [\".pickle\", \".joblib\"])[0]\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_design_info(formula_icar, dataset_file):\n",
    "    \"\"\"Get design info from patsy.\"\"\"\n",
    "    dataset = pd.read_csv(dataset_file)\n",
    "    dataset = dataset.dropna(axis=0)\n",
    "    dataset[\"trial\"] = 1\n",
    "    y, x = dmatrices(formula_icar, dataset, 0, \"drop\")\n",
    "    y_design_info = y.design_info\n",
    "    x_design_info = x.design_info\n",
    "    return (y_design_info, x_design_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rho_file(period_dictionaries, period, model_folder):\n",
    "    period_name = period_dictionaries[period][\"train_period\"]\n",
    "    model_period_folder = model_folder / period_name\n",
    "    rho_file = model_period_folder / \"rho.tif\"\n",
    "    return str(rho_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a343d8",
   "metadata": {},
   "source": [
    "## Apply icar based on period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_for_training = [\n",
    "    \"scale(altitude)\",\n",
    "    \"scale(dist_edge)\",\n",
    "    \"scale(dist_river)\",\n",
    "    \"scale(dist_road)\",\n",
    "    \"scale(dist_town)\",\n",
    "    \"scale(slope)\",\n",
    "    \"C(pa)\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy.build import build_design_matrices\n",
    "from forestatrisk.misc import invlogit, makeblock\n",
    "\n",
    "\n",
    "# predict_binomial_iCAR\n",
    "def predict_binomial_iCAR(model, _x_design_info, new_data, rhos):\n",
    "    \"\"\"Function to return the predictions of a model_binomial_iCAR model.\n",
    "\n",
    "    Function to return the predictions of a model_binomial_iCAR model\n",
    "    for a new data-set. In this function, rho values for spatial cells\n",
    "    are directly provided and not obtained from the model.\n",
    "\n",
    "    :param model: The model_binomial_iCAR model to predict from.\n",
    "    :param new_data: Pandas DataFrame including explicative variables.\n",
    "    :param rhos: Spatial random effects for each observation (row) in new_data.\n",
    "    :return: Predictions (probabilities).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    (x_new,) = build_design_matrices([_x_design_info], new_data)\n",
    "    X_new = x_new[:, :-1]\n",
    "    return invlogit(np.dot(X_new, model[\"betas\"]) + rhos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae117e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from patsy.build import build_design_matrices\n",
    "\n",
    "# Local application imports\n",
    "from forestatrisk.misc import rescale, makeblock\n",
    "\n",
    "\n",
    "# predict_raster\n",
    "def predict_raster_icar(\n",
    "    model,\n",
    "    _x_design_info,\n",
    "    period_dict_files=\"data\",\n",
    "    input_cell_raster=\"output/rho.tif\",\n",
    "    input_forest_raster=\"data/forest.tif\",\n",
    "    output_file=\"predictions.tif\",\n",
    "    blk_rows=128,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Predict the spatial probability of deforestation from a\n",
    "    statistical model.\n",
    "\n",
    "    This function predicts the spatial probability of deforestation\n",
    "    from a statistical model. Computation are done by block and\n",
    "    can be performed on large geographical areas.\n",
    "\n",
    "    :param model: The model (glm, rf) to predict from. Must have a\n",
    "        model.predict_proba() function.\n",
    "    :param _x_design_info: Design matrix information from patsy.\n",
    "    :param var_dir: Directory with rasters (.tif) of explicative variables.\n",
    "    :param input_forest_raster: Path to forest raster (1 for forest).\n",
    "    :param output_file: Name of the output raster file for predictions.\n",
    "    :param blk_rows: If > 0, number of rows for computation by block.\n",
    "    :param verbose: Logical. Whether to print messages or not. Default\n",
    "        to ``True``.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Mask on forest\n",
    "    if verbose:\n",
    "        print(f\"Using {input_forest_raster} file\")\n",
    "    fmaskR = gdal.Open(input_forest_raster)\n",
    "    fmaskB = fmaskR.GetRasterBand(1)\n",
    "\n",
    "    # Landscape variables from forest raster\n",
    "    gt = fmaskR.GetGeoTransform()\n",
    "    ncol = fmaskR.RasterXSize\n",
    "    nrow = fmaskR.RasterYSize\n",
    "    Xmin = gt[0]\n",
    "    Xmax = gt[0] + gt[1] * ncol\n",
    "    Ymin = gt[3] + gt[5] * nrow\n",
    "    Ymax = gt[3]\n",
    "\n",
    "    # Raster list\n",
    "    # Extract keys excluding 'fcc', 'forest' and 'period' and sort them\n",
    "    sorted_keys = sorted(\n",
    "        [key for key in period_dict_files.keys() if key not in [\"period\", \"forest\"]]\n",
    "    )\n",
    "\n",
    "    # Retrieve the corresponding file paths based on the sorted keys\n",
    "    raster_list = [period_dict_files[key] for key in sorted_keys]\n",
    "    raster_list.append(input_cell_raster)\n",
    "    var_names = sorted_keys\n",
    "    var_names.extend([\"rho\", \"fmask\"])\n",
    "    # Make vrt with gdalbuildvrt\n",
    "    if verbose:\n",
    "        print(\"Make virtual raster with variables as raster bands\")\n",
    "    param = gdal.BuildVRTOptions(\n",
    "        resolution=\"user\",\n",
    "        outputBounds=(Xmin, Ymin, Xmax, Ymax),\n",
    "        xRes=gt[1],\n",
    "        yRes=-gt[5],\n",
    "        separate=True,\n",
    "    )\n",
    "    rand_uuid = uuid.uuid4()\n",
    "    vrt_file = f\"/vsimem/var_{rand_uuid}.vrt\"\n",
    "    cback = gdal.TermProgress_nocb if verbose else 0\n",
    "    gdal.BuildVRT(vrt_file, raster_list, options=param, callback=cback)\n",
    "    stack = gdal.Open(vrt_file)\n",
    "    nband = stack.RasterCount\n",
    "    proj = stack.GetProjection()\n",
    "    # List of nodata values\n",
    "    bandND = np.zeros(nband)\n",
    "    for k in range(nband):\n",
    "        band = stack.GetRasterBand(k + 1)\n",
    "        bandND[k] = band.GetNoDataValue()\n",
    "        if (bandND[k] is None) or (bandND[k] is np.nan):\n",
    "            print(f\"NoData value is not specified for input raster file {k}\")\n",
    "            sys.exit(1)\n",
    "    bandND = bandND.astype(np.float32)\n",
    "\n",
    "    # Make blocks\n",
    "    blockinfo = makeblock(vrt_file, blk_rows=blk_rows)\n",
    "    nblock = blockinfo[0]\n",
    "    nblock_x = blockinfo[1]\n",
    "    x = blockinfo[3]\n",
    "    y = blockinfo[4]\n",
    "    nx = blockinfo[5]\n",
    "    ny = blockinfo[6]\n",
    "    if verbose:\n",
    "        print(f\"Divide region in {nblock} blocks\")\n",
    "\n",
    "    # Raster of predictions\n",
    "    if verbose:\n",
    "        print(\"Create a raster file on disk for projections\")\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    try:\n",
    "        os.remove(output_file)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    Pdrv = driver.Create(\n",
    "        output_file,\n",
    "        ncol,\n",
    "        nrow,\n",
    "        1,\n",
    "        gdal.GDT_UInt16,\n",
    "        [\"COMPRESS=DEFLATE\", \"PREDICTOR=2\", \"BIGTIFF=YES\"],\n",
    "    )\n",
    "    Pdrv.SetGeoTransform(gt)\n",
    "    Pdrv.SetProjection(proj)\n",
    "    Pband = Pdrv.GetRasterBand(1)\n",
    "    Pband.SetNoDataValue(0)\n",
    "\n",
    "    # Predict by block\n",
    "    # Message\n",
    "    if verbose:\n",
    "        print(\"Predict deforestation probability by block\")\n",
    "    # Loop on blocks of data\n",
    "    for b in range(nblock):\n",
    "        # Position in 1D-arrays\n",
    "        px = b % nblock_x\n",
    "        py = b // nblock_x\n",
    "        # Number of pixels\n",
    "        npix = nx[px] * ny[py]\n",
    "        # Data for one block of the stack (shape = (nband,nrow,ncol))\n",
    "        data = stack.ReadAsArray(x[px], y[py], nx[px], ny[py])\n",
    "        data = data.astype(float)\n",
    "        # Replace ND values with -9999\n",
    "        for i in range(nband):\n",
    "            data[i][np.nonzero(data[i] == bandND[i])] = -9999\n",
    "        # Forest mask\n",
    "        fmaskA = fmaskB.ReadAsArray(x[px], y[py], nx[px], ny[py])\n",
    "        fmaskA = fmaskA.astype(float)  # From uint to float\n",
    "        fmaskA[np.nonzero(fmaskA != 1)] = -9999\n",
    "        fmaskA = fmaskA[np.newaxis, :, :]\n",
    "        # Concatenate forest mask with stack\n",
    "        data = np.concatenate((data, fmaskA), axis=0)\n",
    "        # Transpose and reshape to 2D array\n",
    "        data = data.transpose(1, 2, 0)\n",
    "        data = data.reshape(npix, nband + 1)\n",
    "        # Observations without NA\n",
    "        w = np.nonzero(~(data == -9999).any(axis=1))\n",
    "        # Remove observations with NA\n",
    "        data = data[w]\n",
    "        # Transform into a pandas DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        df.columns = var_names\n",
    "        # Add fake cell column for _x_design_info\n",
    "        df[\"cell\"] = 0\n",
    "        # Predict\n",
    "        pred = np.zeros(npix)  # Initialize with nodata value (0)\n",
    "        if len(w[0]) > 0:\n",
    "            # Get predictions into an array\n",
    "            p = predict_binomial_iCAR(\n",
    "                model, _x_design_info, new_data=df, rhos=data[:, -2]\n",
    "            )\n",
    "            # Rescale and return to pred\n",
    "            pred[w] = rescale(p)\n",
    "        # Assign prediction to raster\n",
    "        pred = pred.reshape(ny[py], nx[px])\n",
    "        Pband.WriteArray(pred, x[px], y[py])\n",
    "\n",
    "    # Compute statistics\n",
    "    if verbose:\n",
    "        print(\"Compute statistics\")\n",
    "    Pband.FlushCache()  # Write cache data to disk\n",
    "    Pband.ComputeStatistics(False)\n",
    "\n",
    "    # Dereference driver\n",
    "    Pband = None\n",
    "    del Pdrv\n",
    "\n",
    "\n",
    "# End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "from patsy import dmatrices\n",
    "import forestatrisk\n",
    "\n",
    "\n",
    "def apply_icar_period(\n",
    "    period_dictionaries,\n",
    "    period,\n",
    "    model_folder,\n",
    "    processed_data_folder,\n",
    "    sampling_folder,\n",
    "):\n",
    "    period_dictionary = period_dictionaries[period]\n",
    "    period_output_folder = model_folder / period\n",
    "    if not os.path.exists(period_output_folder):\n",
    "        os.makedirs(period_output_folder)\n",
    "    prediction_output = period_output_folder / f\"icar_{period_dictionary['period']}.tif\"\n",
    "\n",
    "    # Variables\n",
    "    model = get_trained_model(period_dictionaries, period, model_folder)\n",
    "    # Load model\n",
    "    with open(model, \"rb\") as file:\n",
    "        model_f = pickle.load(file)\n",
    "\n",
    "    formula = model_f.get(\"formula\")\n",
    "    betas = model_f[\"betas\"]\n",
    "    input_raster_files = filter_out_ipynb_checkpoints(\n",
    "        list_files_by_extension(processed_data_folder, [\".tiff\", \".tif\"])\n",
    "    )\n",
    "    variable_files = get_period_variable_files(\n",
    "        input_raster_files, period_dictionaries, period\n",
    "    )\n",
    "    forest_raster = variable_files[\"forest\"]\n",
    "    samples = get_samples_for_period(period, sampling_folder)\n",
    "    input_cell_raster = get_rho_file(period_dictionaries, period, model_folder)\n",
    "    (y_design_info, x_design_info) = get_design_info(formula, samples)\n",
    "    time_interval = period_dictionary[\"time_interval\"]\n",
    "    predict_raster_icar(\n",
    "        model_f,\n",
    "        x_design_info,\n",
    "        variable_files,\n",
    "        input_cell_raster,\n",
    "        forest_raster,\n",
    "        prediction_output,\n",
    "        blk_rows=128,\n",
    "        verbose=True,\n",
    "    )\n",
    "    # defrate_per_cat\n",
    "    print(\"Calculate deforestation rate per cathegory\")\n",
    "    defrate_output = str(\n",
    "        period_output_folder / f\"defrate_cat_icar_{period_dictionary['period']}.csv\"\n",
    "    )\n",
    "    forestatrisk.defrate_per_cat(\n",
    "        forest_change_file,\n",
    "        str(prediction_output),\n",
    "        time_interval,\n",
    "        period,\n",
    "        defrate_output,\n",
    "        128,\n",
    "        False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_predict_calibration = apply_icar_period(\n",
    "    period_dictionaries,\n",
    "    \"calibration\",\n",
    "    icar_model,\n",
    "    processed_data_folder,\n",
    "    sampling_folder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6008a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_predict_validation = apply_icar_period(\n",
    "    period_dictionaries,\n",
    "    \"validation\",\n",
    "    icar_model,\n",
    "    processed_data_folder,\n",
    "    sampling_folder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_predict_validation = apply_icar_period(\n",
    "    period_dictionaries,\n",
    "    \"historical\",\n",
    "    icar_model,\n",
    "    processed_data_folder,\n",
    "    sampling_folder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_predict_forecast = apply_icar_period(\n",
    "    period_dictionaries,\n",
    "    \"forecast\",\n",
    "    icar_model,\n",
    "    processed_data_folder,\n",
    "    sampling_folder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2bdff-c950-409c-b07d-a7e127bd722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic variable selection\n",
    "# if self.varselection:\n",
    "#                 # Run model while there is non-significant variables\n",
    "#                 var_remove = True\n",
    "#                 while np.any(var_remove):\n",
    "#                     # Formula\n",
    "#                     right_part = \" + \".join(variables) + \" + cell\"\n",
    "#                     left_part = \"I(1-fcc) + trial ~ \"\n",
    "#                     formula = left_part + right_part\n",
    "#                     # Model\n",
    "#                     mod_icar = far.model_binomial_iCAR(\n",
    "#                         # Observations\n",
    "#                         suitability_formula=formula,\n",
    "#                         data=dataset,\n",
    "#                         # Spatial structure\n",
    "#                         n_neighbors=nneigh, neighbors=adj,\n",
    "#                         # Priors\n",
    "#                         priorVrho=self.prior_vrho,\n",
    "#                         # Chains\n",
    "#                         burnin=1000, mcmc=1000, thin=1,\n",
    "#                         # Starting values\n",
    "#                         beta_start=self.beta_start,\n",
    "#                         # Verbose = False for QGIS task\n",
    "#                         verbose=0)\n",
    "#                     # Ecological and statistical significance\n",
    "#                     effects = mod_icar.betas[1:]\n",
    "#                     positive_effects = effects >= 0\n",
    "#                     var_remove = positive_effects\n",
    "#                     var_keep = np.logical_not(var_remove)\n",
    "#                     variables = variables[var_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f5ecd-e8b5-4545-b8f9-fdf8574b3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save xy design\n",
    "# def get_design_info(formula_icar, dataset_file):\n",
    "#     \"\"\"Get design info from patsy.\"\"\"\n",
    "#     dataset = pd.read_csv(dataset_file)\n",
    "#     dataset = dataset.dropna(axis=0)\n",
    "#     dataset[\"trial\"] = 1\n",
    "#     y, x = dmatrices(formula_icar, dataset, 0, \"drop\")\n",
    "#     y_design_info = y.design_info\n",
    "#     x_design_info = x.design_info\n",
    "#     print(x_design_info)\n",
    "#     return (y_design_info, x_design_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac2934-56ce-499e-8179-f110696edbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null model to compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed52f8-462c-4e62-a5e5-f0ee013b33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result table\n",
    "# mod_dev = pd.DataFrame(\n",
    "#     {\"model\": [\"null\", \"glm\", \"rf\", \"icar\", \"full\"],\n",
    "#      \"deviance\": dev})\n",
    "# perc = 100*(1-mod_dev.deviance/deviance_null)\n",
    "# mod_dev[\"perc\"] = perc\n",
    "# mod_dev = mod_dev.round(0)\n",
    "# ofile = opj(self.outdir, \"model_deviances.csv\")\n",
    "# mod_dev.to_csv(ofile, header=True, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deforisk-jupyter-nb (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
